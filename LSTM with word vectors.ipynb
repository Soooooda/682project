{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## pre-pocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "import seaborn as sb\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "import matplotlib.pyplot as plt\n",
    "from numpy import random\n",
    "import gensim\n",
    "import nltk\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.ensemble import AdaBoostRegressor\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from gensim.models import Doc2Vec\n",
    "from gensim.models.doc2vec import TaggedDocument\n",
    "from gensim.models import Word2Vec\n",
    "from sklearn.svm import SVC\n",
    "import torch\n",
    "import torch.utils.data as Data\n",
    "import torchvision"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = \"dataset/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(path+\"USvideos.csv\")\n",
    "df = df.assign(country=\"US\")\n",
    "df['trending_date'] = pd.to_datetime(df['trending_date'], format='%y.%d.%m')  \n",
    "df.trending_date = df.trending_date.dt.date   \n",
    "df['publish_time'] = pd.to_datetime(df['publish_time'], format='%Y-%m-%dT%H:%M:%S.%fZ')\n",
    "df=df.assign(publish_date=df['publish_time'].dt.date)\n",
    "df['publish_time'] = df['publish_time'].dt.time\n",
    "\n",
    "###导入category名称###\n",
    "df=df.assign(cat_name='a')\n",
    "id_to_category = {}\n",
    "file=path+'US_category_id.json'\n",
    "with open(file, 'r') as f:\n",
    "    data=json.load(f)\n",
    "    for category in data['items']:\n",
    "        id_to_category[category['id']] = category['snippet']['title']\n",
    "print(id_to_category)\n",
    "###实际上每个国家的category id-name 字典是一样的\n",
    "df['category_id'] = df['category_id'].astype(str)\n",
    "df.insert(4, 'category', df['category_id'].map(id_to_category))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clean Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x189e8fd0288>"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "df_summary = df.describe(include=\"all\")\n",
    "\n",
    "# Use heatmap to check missing data\n",
    "sb.heatmap(df_summary.isnull(), yticklabels=False, cbar=False, cmap='viridis')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "description 7\n",
      "views 3\n",
      "Unnamed: 2 10\n",
      "Unnamed: 3 10\n",
      "Unnamed: 4 10\n",
      "Unnamed: 5 10\n",
      "Unnamed: 6 10\n",
      "Unnamed: 7 10\n",
      "Unnamed: 8 10\n",
      "Unnamed: 9 10\n",
      "Unnamed: 10 10\n",
      "Unnamed: 11 10\n",
      "Unnamed: 12 10\n",
      "Unnamed: 13 10\n",
      "Unnamed: 14 10\n",
      "Unnamed: 15 10\n",
      "Unnamed: 16 10\n",
      "Unnamed: 17 10\n",
      "Unnamed: 18 10\n",
      "Unnamed: 19 10\n",
      "Unnamed: 20 10\n",
      "Unnamed: 21 3\n",
      "Unnamed: 22 7\n",
      "Unnamed: 23 7\n",
      "Unnamed: 24 7\n",
      "Unnamed: 25 7\n",
      "Unnamed: 26 7\n",
      "country 7\n"
     ]
    }
   ],
   "source": [
    "# See counts of missing value\n",
    "for c in df_summary.columns:\n",
    "    print(c,np.sum(df_summary[c].isnull()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\n",
    "df.sample(5)\n",
    "# Replace missing data\n",
    "df_summary['description'].fillna(\"\", inplace=True)\n",
    "# df_summary['title'].fillna(\"\", inplace=True)\n",
    "# df_summary['category_id'].fillna(0, inplace=True)\n",
    "\n",
    "\n",
    "# for k,v in df_summary.items():\n",
    "#     print(type(df_summary[k]))\n",
    "#     print(\"hhh\")\n",
    "#     df_summary[k].fillna(int(df_summary[k].mean()), inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x189cd72ae88>"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Drop a column most value are missing\n",
    "# df_summary.drop(['thumbnail_link'], axis=1, inplace=True)\n",
    "sb.heatmap(df_summary.isnull(), yticklabels=False, cbar=False, cmap='viridis')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# normalize\n",
    "# Import the libraries\n",
    "\n",
    "# plt.plot(df['views'],df['view'])\n",
    "# matplotlib histogram\n",
    "plt.hist(df['views'], color = 'blue', edgecolor = 'black',\n",
    "         bins = int(30))\n",
    "\n",
    "# # seaborn histogram\n",
    "# sb.distplot(df['view'], hist=True, kde=False, \n",
    "#              bins=int(180/5), color = 'blue',\n",
    "#              hist_kws={'edgecolor':'black'})\n",
    "# Add labels\n",
    "plt.title('Histogram')\n",
    "plt.xlabel('views')\n",
    "plt.ylabel('number')\n",
    "\n",
    "# np.sum(df['views']>2.35e6)\n",
    "y_train = np.zeros_like(df['views'])\n",
    "y_train[df['views']>7e5] = 1\n",
    "y_train[df['views']>2.35e6] = 2\n",
    "# y_train = df['views']\n",
    "# y_train.shape\n",
    "# np.median(df['views'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Luckily the priests were there.\\\\nIf you liked this clip check out the rest of Gordon's channels:\\\\n\\\\nhttp://www.youtube.com/gordonramsay\\\\nhttp://www.youtube.com/thefword\\\\nhttp://www.youtube.com/kitchennightmares\\\\n\\\\nMore Gordon Ramsay:\\\\nWebsite: http://www.gordonramsay.com\\\\nFacebook: http://www.facebook.com/GordonRamsay01\\\\nTwitter: http://www.twitter.com/GordonRamsay\""
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# df['description']\n",
    "# np.arange(1,df.size(),1)\n",
    "# df['views']>2e8\n",
    "df['description'][1435]#.idxmin(axis = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4073"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data, test_data = train_test_split(df, test_size=0.1, random_state=42)\n",
    "train_X, test_X = train_test_split(df, test_size=0.1, random_state=42)\n",
    "train_y, test_y = train_test_split(y_train, test_size=0.1, random_state=42)\n",
    "len(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(40726,)"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['views'].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Big of Words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_text(text):\n",
    "    tokens = []\n",
    "    for sent in nltk.sent_tokenize(text):\n",
    "        for word in nltk.word_tokenize(sent):\n",
    "            if len(word) < 2:\n",
    "                continue\n",
    "            tokens.append(word.lower())\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 1min 41s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# training\n",
    "count_vectorizer = CountVectorizer(\n",
    "    analyzer=\"word\", tokenizer=nltk.word_tokenize,\n",
    "    preprocessor=None, stop_words='english', max_features=3000) \n",
    "train_data_features = count_vectorizer.fit_transform(train_data['description'].values.astype('U')) #3000*train_feature\n",
    "test_data_features = count_vectorizer.transform(test_data['description'].values.astype('U')) #3000*train_feature"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Word Vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 36 s\n"
     ]
    }
   ],
   "source": [
    "%%time \n",
    "import gensim.downloader as api\n",
    "\n",
    "info = api.info()  # show info about available models/datasets\n",
    "model = api.load(\"glove-twitter-25\")  # download the model and return as object ready for use\n",
    "# model.most_similar(\"cat\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def word_averaging(wv, words):\n",
    "    all_words, mean = set(), []\n",
    "    \n",
    "    for word in words:\n",
    "        if isinstance(word, np.ndarray):\n",
    "            mean.append(word)\n",
    "        elif word in wv.vocab:\n",
    "#             print(wv.vocab[word].index)\n",
    "            mean.append(wv.syn0norm[wv.vocab[word].index])\n",
    "            all_words.add(wv.vocab[word].index)\n",
    "\n",
    "    if not mean:\n",
    "#         logging.warning(\"cannot compute similarity with no input %s\", words)\n",
    "        # FIXME: remove these examples in pre-processing\n",
    "        return np.zeros(25,)\n",
    "\n",
    "    mean = gensim.matutils.unitvec(np.array(mean).mean(axis=0)).astype(np.float64)\n",
    "    return mean\n",
    "\n",
    "def  word_averaging_list(wv, text_list):\n",
    "    return np.vstack([word_averaging(wv, review) for review in text_list ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\gx\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:4: DeprecationWarning: Call to deprecated `syn0norm` (Attribute will be removed in 4.0.0, use self.vectors_norm instead).\n",
      "  after removing the cwd from sys.path.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(1193514, 25)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from itertools import islice\n",
    "list(islice(model.vocab, 13000, 13020))\n",
    "model.most_similar(\"cat\")\n",
    "model.syn0norm.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def w2v_tokenize_text(text):\n",
    "    tokens = []\n",
    "    if not text:\n",
    "        print('The text to be tokenized is a None type. Defaulting to blank string.')\n",
    "        text = ''\n",
    "    for sent in nltk.sent_tokenize(text):\n",
    "        for word in nltk.word_tokenize(sent):\n",
    "            if len(word) < 2:\n",
    "                continue\n",
    "            tokens.append(word)\n",
    "    return tokens\n",
    "\n",
    "# def custom_tokenize(text):\n",
    "#     if not text:\n",
    "#         print('The text to be tokenized is a None type. Defaulting to blank string.')\n",
    "#         text = ''\n",
    "#     return word_tokenize(text)\n",
    "\n",
    "def tokenize_text(text):\n",
    "    tokens = []\n",
    "    for sent in nltk.sent_tokenize(text):\n",
    "        for word in nltk.word_tokenize(sent):\n",
    "            if len(word) < 2:\n",
    "                continue\n",
    "            tokens.append(word.lower())\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The text to be tokenized is a None type. Defaulting to blank string.\n",
      "The text to be tokenized is a None type. Defaulting to blank string.\n",
      "The text to be tokenized is a None type. Defaulting to blank string.\n",
      "The text to be tokenized is a None type. Defaulting to blank string.\n",
      "The text to be tokenized is a None type. Defaulting to blank string.\n",
      "The text to be tokenized is a None type. Defaulting to blank string.\n",
      "The text to be tokenized is a None type. Defaulting to blank string.\n",
      "The text to be tokenized is a None type. Defaulting to blank string.\n",
      "The text to be tokenized is a None type. Defaulting to blank string.\n",
      "The text to be tokenized is a None type. Defaulting to blank string.\n",
      "The text to be tokenized is a None type. Defaulting to blank string.\n",
      "The text to be tokenized is a None type. Defaulting to blank string.\n",
      "The text to be tokenized is a None type. Defaulting to blank string.\n",
      "The text to be tokenized is a None type. Defaulting to blank string.\n",
      "The text to be tokenized is a None type. Defaulting to blank string.\n",
      "The text to be tokenized is a None type. Defaulting to blank string.\n",
      "The text to be tokenized is a None type. Defaulting to blank string.\n",
      "The text to be tokenized is a None type. Defaulting to blank string.\n",
      "The text to be tokenized is a None type. Defaulting to blank string.\n",
      "The text to be tokenized is a None type. Defaulting to blank string.\n",
      "The text to be tokenized is a None type. Defaulting to blank string.\n",
      "The text to be tokenized is a None type. Defaulting to blank string.\n",
      "The text to be tokenized is a None type. Defaulting to blank string.\n",
      "The text to be tokenized is a None type. Defaulting to blank string.\n",
      "The text to be tokenized is a None type. Defaulting to blank string.\n",
      "The text to be tokenized is a None type. Defaulting to blank string.\n",
      "The text to be tokenized is a None type. Defaulting to blank string.\n",
      "The text to be tokenized is a None type. Defaulting to blank string.\n",
      "The text to be tokenized is a None type. Defaulting to blank string.\n",
      "The text to be tokenized is a None type. Defaulting to blank string.\n",
      "The text to be tokenized is a None type. Defaulting to blank string.\n",
      "The text to be tokenized is a None type. Defaulting to blank string.\n",
      "The text to be tokenized is a None type. Defaulting to blank string.\n",
      "The text to be tokenized is a None type. Defaulting to blank string.\n",
      "The text to be tokenized is a None type. Defaulting to blank string.\n",
      "The text to be tokenized is a None type. Defaulting to blank string.\n",
      "The text to be tokenized is a None type. Defaulting to blank string.\n",
      "The text to be tokenized is a None type. Defaulting to blank string.\n",
      "The text to be tokenized is a None type. Defaulting to blank string.\n",
      "The text to be tokenized is a None type. Defaulting to blank string.\n",
      "The text to be tokenized is a None type. Defaulting to blank string.\n",
      "The text to be tokenized is a None type. Defaulting to blank string.\n",
      "The text to be tokenized is a None type. Defaulting to blank string.\n",
      "The text to be tokenized is a None type. Defaulting to blank string.\n",
      "The text to be tokenized is a None type. Defaulting to blank string.\n",
      "The text to be tokenized is a None type. Defaulting to blank string.\n",
      "The text to be tokenized is a None type. Defaulting to blank string.\n",
      "The text to be tokenized is a None type. Defaulting to blank string.\n",
      "The text to be tokenized is a None type. Defaulting to blank string.\n",
      "The text to be tokenized is a None type. Defaulting to blank string.\n",
      "The text to be tokenized is a None type. Defaulting to blank string.\n",
      "The text to be tokenized is a None type. Defaulting to blank string.\n",
      "The text to be tokenized is a None type. Defaulting to blank string.\n",
      "The text to be tokenized is a None type. Defaulting to blank string.\n",
      "The text to be tokenized is a None type. Defaulting to blank string.\n",
      "The text to be tokenized is a None type. Defaulting to blank string.\n",
      "The text to be tokenized is a None type. Defaulting to blank string.\n",
      "The text to be tokenized is a None type. Defaulting to blank string.\n",
      "The text to be tokenized is a None type. Defaulting to blank string.\n",
      "The text to be tokenized is a None type. Defaulting to blank string.\n",
      "The text to be tokenized is a None type. Defaulting to blank string.\n",
      "The text to be tokenized is a None type. Defaulting to blank string.\n",
      "The text to be tokenized is a None type. Defaulting to blank string.\n",
      "The text to be tokenized is a None type. Defaulting to blank string.\n",
      "The text to be tokenized is a None type. Defaulting to blank string.\n",
      "The text to be tokenized is a None type. Defaulting to blank string.\n",
      "The text to be tokenized is a None type. Defaulting to blank string.\n",
      "The text to be tokenized is a None type. Defaulting to blank string.\n",
      "The text to be tokenized is a None type. Defaulting to blank string.\n",
      "The text to be tokenized is a None type. Defaulting to blank string.\n",
      "The text to be tokenized is a None type. Defaulting to blank string.\n",
      "The text to be tokenized is a None type. Defaulting to blank string.\n",
      "The text to be tokenized is a None type. Defaulting to blank string.\n",
      "The text to be tokenized is a None type. Defaulting to blank string.\n",
      "The text to be tokenized is a None type. Defaulting to blank string.\n",
      "The text to be tokenized is a None type. Defaulting to blank string.\n",
      "The text to be tokenized is a None type. Defaulting to blank string.\n",
      "The text to be tokenized is a None type. Defaulting to blank string.\n",
      "The text to be tokenized is a None type. Defaulting to blank string.\n",
      "The text to be tokenized is a None type. Defaulting to blank string.\n",
      "The text to be tokenized is a None type. Defaulting to blank string.\n",
      "The text to be tokenized is a None type. Defaulting to blank string.\n",
      "The text to be tokenized is a None type. Defaulting to blank string.\n",
      "The text to be tokenized is a None type. Defaulting to blank string.\n",
      "The text to be tokenized is a None type. Defaulting to blank string.\n",
      "The text to be tokenized is a None type. Defaulting to blank string.\n",
      "The text to be tokenized is a None type. Defaulting to blank string.\n",
      "The text to be tokenized is a None type. Defaulting to blank string.\n",
      "The text to be tokenized is a None type. Defaulting to blank string.\n",
      "The text to be tokenized is a None type. Defaulting to blank string.\n",
      "The text to be tokenized is a None type. Defaulting to blank string.\n",
      "The text to be tokenized is a None type. Defaulting to blank string.\n",
      "The text to be tokenized is a None type. Defaulting to blank string.\n",
      "The text to be tokenized is a None type. Defaulting to blank string.\n",
      "The text to be tokenized is a None type. Defaulting to blank string.\n",
      "The text to be tokenized is a None type. Defaulting to blank string.\n",
      "The text to be tokenized is a None type. Defaulting to blank string.\n",
      "The text to be tokenized is a None type. Defaulting to blank string.\n",
      "The text to be tokenized is a None type. Defaulting to blank string.\n",
      "The text to be tokenized is a None type. Defaulting to blank string.\n",
      "The text to be tokenized is a None type. Defaulting to blank string.\n",
      "The text to be tokenized is a None type. Defaulting to blank string.\n",
      "The text to be tokenized is a None type. Defaulting to blank string.\n",
      "The text to be tokenized is a None type. Defaulting to blank string.\n",
      "The text to be tokenized is a None type. Defaulting to blank string.\n",
      "The text to be tokenized is a None type. Defaulting to blank string.\n",
      "The text to be tokenized is a None type. Defaulting to blank string.\n",
      "The text to be tokenized is a None type. Defaulting to blank string.\n",
      "The text to be tokenized is a None type. Defaulting to blank string.\n",
      "The text to be tokenized is a None type. Defaulting to blank string.\n",
      "The text to be tokenized is a None type. Defaulting to blank string.\n",
      "The text to be tokenized is a None type. Defaulting to blank string.\n",
      "The text to be tokenized is a None type. Defaulting to blank string.\n",
      "The text to be tokenized is a None type. Defaulting to blank string.\n",
      "The text to be tokenized is a None type. Defaulting to blank string.\n",
      "The text to be tokenized is a None type. Defaulting to blank string.\n",
      "The text to be tokenized is a None type. Defaulting to blank string.\n",
      "The text to be tokenized is a None type. Defaulting to blank string.\n",
      "The text to be tokenized is a None type. Defaulting to blank string.\n",
      "The text to be tokenized is a None type. Defaulting to blank string.\n",
      "The text to be tokenized is a None type. Defaulting to blank string.\n",
      "The text to be tokenized is a None type. Defaulting to blank string.\n",
      "The text to be tokenized is a None type. Defaulting to blank string.\n",
      "The text to be tokenized is a None type. Defaulting to blank string.\n",
      "The text to be tokenized is a None type. Defaulting to blank string.\n",
      "The text to be tokenized is a None type. Defaulting to blank string.\n",
      "The text to be tokenized is a None type. Defaulting to blank string.\n",
      "The text to be tokenized is a None type. Defaulting to blank string.\n",
      "The text to be tokenized is a None type. Defaulting to blank string.\n",
      "The text to be tokenized is a None type. Defaulting to blank string.\n",
      "The text to be tokenized is a None type. Defaulting to blank string.\n",
      "The text to be tokenized is a None type. Defaulting to blank string.\n",
      "The text to be tokenized is a None type. Defaulting to blank string.\n",
      "The text to be tokenized is a None type. Defaulting to blank string.\n",
      "The text to be tokenized is a None type. Defaulting to blank string.\n",
      "The text to be tokenized is a None type. Defaulting to blank string.\n",
      "The text to be tokenized is a None type. Defaulting to blank string.\n",
      "The text to be tokenized is a None type. Defaulting to blank string.\n",
      "The text to be tokenized is a None type. Defaulting to blank string.\n",
      "The text to be tokenized is a None type. Defaulting to blank string.\n",
      "The text to be tokenized is a None type. Defaulting to blank string.\n",
      "The text to be tokenized is a None type. Defaulting to blank string.\n",
      "The text to be tokenized is a None type. Defaulting to blank string.\n",
      "The text to be tokenized is a None type. Defaulting to blank string.\n",
      "The text to be tokenized is a None type. Defaulting to blank string.\n",
      "The text to be tokenized is a None type. Defaulting to blank string.\n",
      "The text to be tokenized is a None type. Defaulting to blank string.\n",
      "The text to be tokenized is a None type. Defaulting to blank string.\n",
      "The text to be tokenized is a None type. Defaulting to blank string.\n",
      "The text to be tokenized is a None type. Defaulting to blank string.\n",
      "The text to be tokenized is a None type. Defaulting to blank string.\n",
      "The text to be tokenized is a None type. Defaulting to blank string.\n",
      "The text to be tokenized is a None type. Defaulting to blank string.\n",
      "The text to be tokenized is a None type. Defaulting to blank string.\n",
      "The text to be tokenized is a None type. Defaulting to blank string.\n",
      "The text to be tokenized is a None type. Defaulting to blank string.\n",
      "The text to be tokenized is a None type. Defaulting to blank string.\n",
      "The text to be tokenized is a None type. Defaulting to blank string.\n",
      "The text to be tokenized is a None type. Defaulting to blank string.\n",
      "The text to be tokenized is a None type. Defaulting to blank string.\n",
      "The text to be tokenized is a None type. Defaulting to blank string.\n",
      "The text to be tokenized is a None type. Defaulting to blank string.\n",
      "The text to be tokenized is a None type. Defaulting to blank string.\n",
      "The text to be tokenized is a None type. Defaulting to blank string.\n",
      "The text to be tokenized is a None type. Defaulting to blank string.\n",
      "The text to be tokenized is a None type. Defaulting to blank string.\n",
      "The text to be tokenized is a None type. Defaulting to blank string.\n",
      "The text to be tokenized is a None type. Defaulting to blank string.\n",
      "The text to be tokenized is a None type. Defaulting to blank string.\n",
      "The text to be tokenized is a None type. Defaulting to blank string.\n",
      "The text to be tokenized is a None type. Defaulting to blank string.\n",
      "The text to be tokenized is a None type. Defaulting to blank string.\n",
      "The text to be tokenized is a None type. Defaulting to blank string.\n",
      "The text to be tokenized is a None type. Defaulting to blank string.\n",
      "The text to be tokenized is a None type. Defaulting to blank string.\n",
      "The text to be tokenized is a None type. Defaulting to blank string.\n",
      "The text to be tokenized is a None type. Defaulting to blank string.\n",
      "The text to be tokenized is a None type. Defaulting to blank string.\n",
      "The text to be tokenized is a None type. Defaulting to blank string.\n",
      "The text to be tokenized is a None type. Defaulting to blank string.\n",
      "The text to be tokenized is a None type. Defaulting to blank string.\n",
      "The text to be tokenized is a None type. Defaulting to blank string.\n",
      "The text to be tokenized is a None type. Defaulting to blank string.\n",
      "The text to be tokenized is a None type. Defaulting to blank string.\n",
      "The text to be tokenized is a None type. Defaulting to blank string.\n",
      "The text to be tokenized is a None type. Defaulting to blank string.\n",
      "The text to be tokenized is a None type. Defaulting to blank string.\n",
      "The text to be tokenized is a None type. Defaulting to blank string.\n",
      "The text to be tokenized is a None type. Defaulting to blank string.\n",
      "The text to be tokenized is a None type. Defaulting to blank string.\n",
      "The text to be tokenized is a None type. Defaulting to blank string.\n",
      "The text to be tokenized is a None type. Defaulting to blank string.\n",
      "The text to be tokenized is a None type. Defaulting to blank string.\n",
      "The text to be tokenized is a None type. Defaulting to blank string.\n",
      "The text to be tokenized is a None type. Defaulting to blank string.\n",
      "The text to be tokenized is a None type. Defaulting to blank string.\n",
      "The text to be tokenized is a None type. Defaulting to blank string.\n",
      "The text to be tokenized is a None type. Defaulting to blank string.\n",
      "The text to be tokenized is a None type. Defaulting to blank string.\n",
      "The text to be tokenized is a None type. Defaulting to blank string.\n",
      "The text to be tokenized is a None type. Defaulting to blank string.\n",
      "The text to be tokenized is a None type. Defaulting to blank string.\n",
      "The text to be tokenized is a None type. Defaulting to blank string.\n",
      "The text to be tokenized is a None type. Defaulting to blank string.\n",
      "The text to be tokenized is a None type. Defaulting to blank string.\n",
      "The text to be tokenized is a None type. Defaulting to blank string.\n",
      "The text to be tokenized is a None type. Defaulting to blank string.\n",
      "The text to be tokenized is a None type. Defaulting to blank string.\n",
      "The text to be tokenized is a None type. Defaulting to blank string.\n",
      "The text to be tokenized is a None type. Defaulting to blank string.\n",
      "The text to be tokenized is a None type. Defaulting to blank string.\n",
      "The text to be tokenized is a None type. Defaulting to blank string.\n",
      "The text to be tokenized is a None type. Defaulting to blank string.\n",
      "The text to be tokenized is a None type. Defaulting to blank string.\n",
      "The text to be tokenized is a None type. Defaulting to blank string.\n",
      "The text to be tokenized is a None type. Defaulting to blank string.\n",
      "The text to be tokenized is a None type. Defaulting to blank string.\n",
      "The text to be tokenized is a None type. Defaulting to blank string.\n",
      "The text to be tokenized is a None type. Defaulting to blank string.\n",
      "The text to be tokenized is a None type. Defaulting to blank string.\n",
      "The text to be tokenized is a None type. Defaulting to blank string.\n",
      "The text to be tokenized is a None type. Defaulting to blank string.\n",
      "The text to be tokenized is a None type. Defaulting to blank string.\n",
      "The text to be tokenized is a None type. Defaulting to blank string.\n",
      "The text to be tokenized is a None type. Defaulting to blank string.\n",
      "The text to be tokenized is a None type. Defaulting to blank string.\n",
      "The text to be tokenized is a None type. Defaulting to blank string.\n",
      "The text to be tokenized is a None type. Defaulting to blank string.\n",
      "The text to be tokenized is a None type. Defaulting to blank string.\n",
      "The text to be tokenized is a None type. Defaulting to blank string.\n",
      "The text to be tokenized is a None type. Defaulting to blank string.\n",
      "The text to be tokenized is a None type. Defaulting to blank string.\n",
      "The text to be tokenized is a None type. Defaulting to blank string.\n",
      "The text to be tokenized is a None type. Defaulting to blank string.\n",
      "The text to be tokenized is a None type. Defaulting to blank string.\n",
      "The text to be tokenized is a None type. Defaulting to blank string.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The text to be tokenized is a None type. Defaulting to blank string.\n",
      "The text to be tokenized is a None type. Defaulting to blank string.\n",
      "The text to be tokenized is a None type. Defaulting to blank string.\n",
      "The text to be tokenized is a None type. Defaulting to blank string.\n",
      "The text to be tokenized is a None type. Defaulting to blank string.\n",
      "The text to be tokenized is a None type. Defaulting to blank string.\n",
      "The text to be tokenized is a None type. Defaulting to blank string.\n",
      "The text to be tokenized is a None type. Defaulting to blank string.\n",
      "The text to be tokenized is a None type. Defaulting to blank string.\n",
      "The text to be tokenized is a None type. Defaulting to blank string.\n",
      "The text to be tokenized is a None type. Defaulting to blank string.\n",
      "The text to be tokenized is a None type. Defaulting to blank string.\n",
      "The text to be tokenized is a None type. Defaulting to blank string.\n",
      "The text to be tokenized is a None type. Defaulting to blank string.\n",
      "The text to be tokenized is a None type. Defaulting to blank string.\n",
      "The text to be tokenized is a None type. Defaulting to blank string.\n",
      "The text to be tokenized is a None type. Defaulting to blank string.\n",
      "The text to be tokenized is a None type. Defaulting to blank string.\n",
      "The text to be tokenized is a None type. Defaulting to blank string.\n",
      "The text to be tokenized is a None type. Defaulting to blank string.\n",
      "The text to be tokenized is a None type. Defaulting to blank string.\n",
      "The text to be tokenized is a None type. Defaulting to blank string.\n",
      "The text to be tokenized is a None type. Defaulting to blank string.\n",
      "The text to be tokenized is a None type. Defaulting to blank string.\n",
      "The text to be tokenized is a None type. Defaulting to blank string.\n",
      "The text to be tokenized is a None type. Defaulting to blank string.\n",
      "The text to be tokenized is a None type. Defaulting to blank string.\n",
      "The text to be tokenized is a None type. Defaulting to blank string.\n",
      "The text to be tokenized is a None type. Defaulting to blank string.\n",
      "The text to be tokenized is a None type. Defaulting to blank string.\n",
      "The text to be tokenized is a None type. Defaulting to blank string.\n",
      "The text to be tokenized is a None type. Defaulting to blank string.\n",
      "The text to be tokenized is a None type. Defaulting to blank string.\n",
      "The text to be tokenized is a None type. Defaulting to blank string.\n",
      "The text to be tokenized is a None type. Defaulting to blank string.\n",
      "The text to be tokenized is a None type. Defaulting to blank string.\n",
      "The text to be tokenized is a None type. Defaulting to blank string.\n",
      "The text to be tokenized is a None type. Defaulting to blank string.\n",
      "The text to be tokenized is a None type. Defaulting to blank string.\n",
      "The text to be tokenized is a None type. Defaulting to blank string.\n",
      "The text to be tokenized is a None type. Defaulting to blank string.\n",
      "The text to be tokenized is a None type. Defaulting to blank string.\n",
      "The text to be tokenized is a None type. Defaulting to blank string.\n",
      "The text to be tokenized is a None type. Defaulting to blank string.\n",
      "The text to be tokenized is a None type. Defaulting to blank string.\n",
      "The text to be tokenized is a None type. Defaulting to blank string.\n",
      "The text to be tokenized is a None type. Defaulting to blank string.\n",
      "The text to be tokenized is a None type. Defaulting to blank string.\n",
      "The text to be tokenized is a None type. Defaulting to blank string.\n",
      "The text to be tokenized is a None type. Defaulting to blank string.\n",
      "The text to be tokenized is a None type. Defaulting to blank string.\n",
      "The text to be tokenized is a None type. Defaulting to blank string.\n",
      "The text to be tokenized is a None type. Defaulting to blank string.\n",
      "The text to be tokenized is a None type. Defaulting to blank string.\n",
      "The text to be tokenized is a None type. Defaulting to blank string.\n",
      "The text to be tokenized is a None type. Defaulting to blank string.\n",
      "The text to be tokenized is a None type. Defaulting to blank string.\n",
      "The text to be tokenized is a None type. Defaulting to blank string.\n",
      "The text to be tokenized is a None type. Defaulting to blank string.\n",
      "The text to be tokenized is a None type. Defaulting to blank string.\n",
      "The text to be tokenized is a None type. Defaulting to blank string.\n",
      "The text to be tokenized is a None type. Defaulting to blank string.\n",
      "The text to be tokenized is a None type. Defaulting to blank string.\n",
      "The text to be tokenized is a None type. Defaulting to blank string.\n",
      "The text to be tokenized is a None type. Defaulting to blank string.\n",
      "The text to be tokenized is a None type. Defaulting to blank string.\n",
      "The text to be tokenized is a None type. Defaulting to blank string.\n",
      "The text to be tokenized is a None type. Defaulting to blank string.\n",
      "The text to be tokenized is a None type. Defaulting to blank string.\n",
      "The text to be tokenized is a None type. Defaulting to blank string.\n",
      "The text to be tokenized is a None type. Defaulting to blank string.\n",
      "The text to be tokenized is a None type. Defaulting to blank string.\n",
      "The text to be tokenized is a None type. Defaulting to blank string.\n",
      "The text to be tokenized is a None type. Defaulting to blank string.\n",
      "The text to be tokenized is a None type. Defaulting to blank string.\n",
      "The text to be tokenized is a None type. Defaulting to blank string.\n",
      "The text to be tokenized is a None type. Defaulting to blank string.\n",
      "The text to be tokenized is a None type. Defaulting to blank string.\n",
      "The text to be tokenized is a None type. Defaulting to blank string.\n",
      "The text to be tokenized is a None type. Defaulting to blank string.\n",
      "The text to be tokenized is a None type. Defaulting to blank string.\n",
      "The text to be tokenized is a None type. Defaulting to blank string.\n",
      "The text to be tokenized is a None type. Defaulting to blank string.\n",
      "The text to be tokenized is a None type. Defaulting to blank string.\n",
      "The text to be tokenized is a None type. Defaulting to blank string.\n",
      "The text to be tokenized is a None type. Defaulting to blank string.\n",
      "The text to be tokenized is a None type. Defaulting to blank string.\n",
      "The text to be tokenized is a None type. Defaulting to blank string.\n",
      "The text to be tokenized is a None type. Defaulting to blank string.\n",
      "The text to be tokenized is a None type. Defaulting to blank string.\n",
      "The text to be tokenized is a None type. Defaulting to blank string.\n",
      "The text to be tokenized is a None type. Defaulting to blank string.\n",
      "The text to be tokenized is a None type. Defaulting to blank string.\n",
      "The text to be tokenized is a None type. Defaulting to blank string.\n",
      "The text to be tokenized is a None type. Defaulting to blank string.\n",
      "The text to be tokenized is a None type. Defaulting to blank string.\n",
      "The text to be tokenized is a None type. Defaulting to blank string.\n",
      "The text to be tokenized is a None type. Defaulting to blank string.\n",
      "The text to be tokenized is a None type. Defaulting to blank string.\n",
      "The text to be tokenized is a None type. Defaulting to blank string.\n",
      "The text to be tokenized is a None type. Defaulting to blank string.\n",
      "The text to be tokenized is a None type. Defaulting to blank string.\n",
      "The text to be tokenized is a None type. Defaulting to blank string.\n",
      "The text to be tokenized is a None type. Defaulting to blank string.\n",
      "The text to be tokenized is a None type. Defaulting to blank string.\n",
      "The text to be tokenized is a None type. Defaulting to blank string.\n",
      "The text to be tokenized is a None type. Defaulting to blank string.\n",
      "The text to be tokenized is a None type. Defaulting to blank string.\n",
      "The text to be tokenized is a None type. Defaulting to blank string.\n",
      "The text to be tokenized is a None type. Defaulting to blank string.\n",
      "The text to be tokenized is a None type. Defaulting to blank string.\n",
      "The text to be tokenized is a None type. Defaulting to blank string.\n",
      "The text to be tokenized is a None type. Defaulting to blank string.\n",
      "The text to be tokenized is a None type. Defaulting to blank string.\n",
      "The text to be tokenized is a None type. Defaulting to blank string.\n",
      "The text to be tokenized is a None type. Defaulting to blank string.\n",
      "The text to be tokenized is a None type. Defaulting to blank string.\n",
      "The text to be tokenized is a None type. Defaulting to blank string.\n",
      "The text to be tokenized is a None type. Defaulting to blank string.\n",
      "The text to be tokenized is a None type. Defaulting to blank string.\n",
      "The text to be tokenized is a None type. Defaulting to blank string.\n",
      "The text to be tokenized is a None type. Defaulting to blank string.\n",
      "The text to be tokenized is a None type. Defaulting to blank string.\n",
      "The text to be tokenized is a None type. Defaulting to blank string.\n",
      "The text to be tokenized is a None type. Defaulting to blank string.\n",
      "The text to be tokenized is a None type. Defaulting to blank string.\n",
      "The text to be tokenized is a None type. Defaulting to blank string.\n",
      "The text to be tokenized is a None type. Defaulting to blank string.\n",
      "The text to be tokenized is a None type. Defaulting to blank string.\n",
      "The text to be tokenized is a None type. Defaulting to blank string.\n",
      "The text to be tokenized is a None type. Defaulting to blank string.\n",
      "The text to be tokenized is a None type. Defaulting to blank string.\n",
      "The text to be tokenized is a None type. Defaulting to blank string.\n",
      "The text to be tokenized is a None type. Defaulting to blank string.\n",
      "The text to be tokenized is a None type. Defaulting to blank string.\n",
      "The text to be tokenized is a None type. Defaulting to blank string.\n",
      "The text to be tokenized is a None type. Defaulting to blank string.\n",
      "The text to be tokenized is a None type. Defaulting to blank string.\n",
      "The text to be tokenized is a None type. Defaulting to blank string.\n",
      "The text to be tokenized is a None type. Defaulting to blank string.\n",
      "The text to be tokenized is a None type. Defaulting to blank string.\n",
      "The text to be tokenized is a None type. Defaulting to blank string.\n",
      "The text to be tokenized is a None type. Defaulting to blank string.\n",
      "The text to be tokenized is a None type. Defaulting to blank string.\n",
      "The text to be tokenized is a None type. Defaulting to blank string.\n",
      "The text to be tokenized is a None type. Defaulting to blank string.\n",
      "The text to be tokenized is a None type. Defaulting to blank string.\n",
      "The text to be tokenized is a None type. Defaulting to blank string.\n",
      "The text to be tokenized is a None type. Defaulting to blank string.\n",
      "The text to be tokenized is a None type. Defaulting to blank string.\n",
      "The text to be tokenized is a None type. Defaulting to blank string.\n",
      "The text to be tokenized is a None type. Defaulting to blank string.\n",
      "The text to be tokenized is a None type. Defaulting to blank string.\n",
      "The text to be tokenized is a None type. Defaulting to blank string.\n",
      "The text to be tokenized is a None type. Defaulting to blank string.\n",
      "The text to be tokenized is a None type. Defaulting to blank string.\n",
      "The text to be tokenized is a None type. Defaulting to blank string.\n",
      "The text to be tokenized is a None type. Defaulting to blank string.\n",
      "The text to be tokenized is a None type. Defaulting to blank string.\n",
      "The text to be tokenized is a None type. Defaulting to blank string.\n",
      "The text to be tokenized is a None type. Defaulting to blank string.\n",
      "The text to be tokenized is a None type. Defaulting to blank string.\n",
      "The text to be tokenized is a None type. Defaulting to blank string.\n",
      "The text to be tokenized is a None type. Defaulting to blank string.\n",
      "The text to be tokenized is a None type. Defaulting to blank string.\n",
      "The text to be tokenized is a None type. Defaulting to blank string.\n",
      "The text to be tokenized is a None type. Defaulting to blank string.\n",
      "The text to be tokenized is a None type. Defaulting to blank string.\n",
      "The text to be tokenized is a None type. Defaulting to blank string.\n",
      "The text to be tokenized is a None type. Defaulting to blank string.\n",
      "The text to be tokenized is a None type. Defaulting to blank string.\n",
      "The text to be tokenized is a None type. Defaulting to blank string.\n",
      "The text to be tokenized is a None type. Defaulting to blank string.\n",
      "The text to be tokenized is a None type. Defaulting to blank string.\n",
      "The text to be tokenized is a None type. Defaulting to blank string.\n",
      "The text to be tokenized is a None type. Defaulting to blank string.\n",
      "The text to be tokenized is a None type. Defaulting to blank string.\n",
      "The text to be tokenized is a None type. Defaulting to blank string.\n",
      "The text to be tokenized is a None type. Defaulting to blank string.\n",
      "The text to be tokenized is a None type. Defaulting to blank string.\n",
      "The text to be tokenized is a None type. Defaulting to blank string.\n",
      "The text to be tokenized is a None type. Defaulting to blank string.\n",
      "The text to be tokenized is a None type. Defaulting to blank string.\n",
      "The text to be tokenized is a None type. Defaulting to blank string.\n",
      "The text to be tokenized is a None type. Defaulting to blank string.\n",
      "The text to be tokenized is a None type. Defaulting to blank string.\n",
      "The text to be tokenized is a None type. Defaulting to blank string.\n",
      "The text to be tokenized is a None type. Defaulting to blank string.\n",
      "The text to be tokenized is a None type. Defaulting to blank string.\n",
      "The text to be tokenized is a None type. Defaulting to blank string.\n",
      "The text to be tokenized is a None type. Defaulting to blank string.\n",
      "The text to be tokenized is a None type. Defaulting to blank string.\n",
      "The text to be tokenized is a None type. Defaulting to blank string.\n",
      "The text to be tokenized is a None type. Defaulting to blank string.\n",
      "The text to be tokenized is a None type. Defaulting to blank string.\n",
      "The text to be tokenized is a None type. Defaulting to blank string.\n",
      "The text to be tokenized is a None type. Defaulting to blank string.\n",
      "The text to be tokenized is a None type. Defaulting to blank string.\n",
      "The text to be tokenized is a None type. Defaulting to blank string.\n",
      "The text to be tokenized is a None type. Defaulting to blank string.\n",
      "The text to be tokenized is a None type. Defaulting to blank string.\n",
      "The text to be tokenized is a None type. Defaulting to blank string.\n",
      "The text to be tokenized is a None type. Defaulting to blank string.\n",
      "The text to be tokenized is a None type. Defaulting to blank string.\n",
      "The text to be tokenized is a None type. Defaulting to blank string.\n",
      "The text to be tokenized is a None type. Defaulting to blank string.\n",
      "The text to be tokenized is a None type. Defaulting to blank string.\n",
      "The text to be tokenized is a None type. Defaulting to blank string.\n",
      "The text to be tokenized is a None type. Defaulting to blank string.\n",
      "The text to be tokenized is a None type. Defaulting to blank string.\n",
      "The text to be tokenized is a None type. Defaulting to blank string.\n",
      "The text to be tokenized is a None type. Defaulting to blank string.\n",
      "The text to be tokenized is a None type. Defaulting to blank string.\n",
      "The text to be tokenized is a None type. Defaulting to blank string.\n",
      "The text to be tokenized is a None type. Defaulting to blank string.\n",
      "The text to be tokenized is a None type. Defaulting to blank string.\n",
      "The text to be tokenized is a None type. Defaulting to blank string.\n",
      "The text to be tokenized is a None type. Defaulting to blank string.\n",
      "The text to be tokenized is a None type. Defaulting to blank string.\n",
      "The text to be tokenized is a None type. Defaulting to blank string.\n",
      "The text to be tokenized is a None type. Defaulting to blank string.\n",
      "The text to be tokenized is a None type. Defaulting to blank string.\n",
      "The text to be tokenized is a None type. Defaulting to blank string.\n",
      "The text to be tokenized is a None type. Defaulting to blank string.\n",
      "The text to be tokenized is a None type. Defaulting to blank string.\n",
      "The text to be tokenized is a None type. Defaulting to blank string.\n",
      "The text to be tokenized is a None type. Defaulting to blank string.\n",
      "The text to be tokenized is a None type. Defaulting to blank string.\n",
      "The text to be tokenized is a None type. Defaulting to blank string.\n",
      "The text to be tokenized is a None type. Defaulting to blank string.\n",
      "The text to be tokenized is a None type. Defaulting to blank string.\n",
      "The text to be tokenized is a None type. Defaulting to blank string.\n",
      "The text to be tokenized is a None type. Defaulting to blank string.\n",
      "The text to be tokenized is a None type. Defaulting to blank string.\n",
      "The text to be tokenized is a None type. Defaulting to blank string.\n",
      "The text to be tokenized is a None type. Defaulting to blank string.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The text to be tokenized is a None type. Defaulting to blank string.\n",
      "The text to be tokenized is a None type. Defaulting to blank string.\n",
      "The text to be tokenized is a None type. Defaulting to blank string.\n",
      "The text to be tokenized is a None type. Defaulting to blank string.\n",
      "The text to be tokenized is a None type. Defaulting to blank string.\n",
      "The text to be tokenized is a None type. Defaulting to blank string.\n",
      "The text to be tokenized is a None type. Defaulting to blank string.\n",
      "The text to be tokenized is a None type. Defaulting to blank string.\n",
      "The text to be tokenized is a None type. Defaulting to blank string.\n",
      "The text to be tokenized is a None type. Defaulting to blank string.\n",
      "The text to be tokenized is a None type. Defaulting to blank string.\n",
      "The text to be tokenized is a None type. Defaulting to blank string.\n",
      "The text to be tokenized is a None type. Defaulting to blank string.\n",
      "The text to be tokenized is a None type. Defaulting to blank string.\n",
      "The text to be tokenized is a None type. Defaulting to blank string.\n",
      "The text to be tokenized is a None type. Defaulting to blank string.\n",
      "The text to be tokenized is a None type. Defaulting to blank string.\n",
      "The text to be tokenized is a None type. Defaulting to blank string.\n",
      "The text to be tokenized is a None type. Defaulting to blank string.\n",
      "The text to be tokenized is a None type. Defaulting to blank string.\n",
      "The text to be tokenized is a None type. Defaulting to blank string.\n",
      "The text to be tokenized is a None type. Defaulting to blank string.\n",
      "The text to be tokenized is a None type. Defaulting to blank string.\n",
      "The text to be tokenized is a None type. Defaulting to blank string.\n",
      "The text to be tokenized is a None type. Defaulting to blank string.\n",
      "The text to be tokenized is a None type. Defaulting to blank string.\n",
      "The text to be tokenized is a None type. Defaulting to blank string.\n",
      "The text to be tokenized is a None type. Defaulting to blank string.\n",
      "The text to be tokenized is a None type. Defaulting to blank string.\n",
      "The text to be tokenized is a None type. Defaulting to blank string.\n",
      "The text to be tokenized is a None type. Defaulting to blank string.\n",
      "The text to be tokenized is a None type. Defaulting to blank string.\n",
      "The text to be tokenized is a None type. Defaulting to blank string.\n",
      "The text to be tokenized is a None type. Defaulting to blank string.\n",
      "The text to be tokenized is a None type. Defaulting to blank string.\n",
      "The text to be tokenized is a None type. Defaulting to blank string.\n",
      "The text to be tokenized is a None type. Defaulting to blank string.\n",
      "The text to be tokenized is a None type. Defaulting to blank string.\n",
      "The text to be tokenized is a None type. Defaulting to blank string.\n",
      "The text to be tokenized is a None type. Defaulting to blank string.\n",
      "The text to be tokenized is a None type. Defaulting to blank string.\n",
      "The text to be tokenized is a None type. Defaulting to blank string.\n",
      "The text to be tokenized is a None type. Defaulting to blank string.\n",
      "The text to be tokenized is a None type. Defaulting to blank string.\n",
      "The text to be tokenized is a None type. Defaulting to blank string.\n",
      "The text to be tokenized is a None type. Defaulting to blank string.\n",
      "The text to be tokenized is a None type. Defaulting to blank string.\n",
      "The text to be tokenized is a None type. Defaulting to blank string.\n",
      "The text to be tokenized is a None type. Defaulting to blank string.\n",
      "The text to be tokenized is a None type. Defaulting to blank string.\n",
      "The text to be tokenized is a None type. Defaulting to blank string.\n",
      "The text to be tokenized is a None type. Defaulting to blank string.\n",
      "The text to be tokenized is a None type. Defaulting to blank string.\n",
      "The text to be tokenized is a None type. Defaulting to blank string.\n",
      "The text to be tokenized is a None type. Defaulting to blank string.\n",
      "The text to be tokenized is a None type. Defaulting to blank string.\n",
      "The text to be tokenized is a None type. Defaulting to blank string.\n",
      "The text to be tokenized is a None type. Defaulting to blank string.\n",
      "The text to be tokenized is a None type. Defaulting to blank string.\n",
      "The text to be tokenized is a None type. Defaulting to blank string.\n",
      "The text to be tokenized is a None type. Defaulting to blank string.\n",
      "The text to be tokenized is a None type. Defaulting to blank string.\n",
      "The text to be tokenized is a None type. Defaulting to blank string.\n",
      "The text to be tokenized is a None type. Defaulting to blank string.\n",
      "The text to be tokenized is a None type. Defaulting to blank string.\n",
      "The text to be tokenized is a None type. Defaulting to blank string.\n",
      "The text to be tokenized is a None type. Defaulting to blank string.\n",
      "The text to be tokenized is a None type. Defaulting to blank string.\n",
      "The text to be tokenized is a None type. Defaulting to blank string.\n",
      "The text to be tokenized is a None type. Defaulting to blank string.\n",
      "The text to be tokenized is a None type. Defaulting to blank string.\n",
      "The text to be tokenized is a None type. Defaulting to blank string.\n",
      "The text to be tokenized is a None type. Defaulting to blank string.\n",
      "The text to be tokenized is a None type. Defaulting to blank string.\n",
      "The text to be tokenized is a None type. Defaulting to blank string.\n",
      "The text to be tokenized is a None type. Defaulting to blank string.\n",
      "The text to be tokenized is a None type. Defaulting to blank string.\n",
      "The text to be tokenized is a None type. Defaulting to blank string.\n",
      "The text to be tokenized is a None type. Defaulting to blank string.\n",
      "The text to be tokenized is a None type. Defaulting to blank string.\n",
      "The text to be tokenized is a None type. Defaulting to blank string.\n",
      "The text to be tokenized is a None type. Defaulting to blank string.\n",
      "The text to be tokenized is a None type. Defaulting to blank string.\n",
      "The text to be tokenized is a None type. Defaulting to blank string.\n",
      "The text to be tokenized is a None type. Defaulting to blank string.\n",
      "The text to be tokenized is a None type. Defaulting to blank string.\n",
      "The text to be tokenized is a None type. Defaulting to blank string.\n",
      "The text to be tokenized is a None type. Defaulting to blank string.\n",
      "The text to be tokenized is a None type. Defaulting to blank string.\n",
      "The text to be tokenized is a None type. Defaulting to blank string.\n",
      "The text to be tokenized is a None type. Defaulting to blank string.\n",
      "The text to be tokenized is a None type. Defaulting to blank string.\n",
      "The text to be tokenized is a None type. Defaulting to blank string.\n",
      "The text to be tokenized is a None type. Defaulting to blank string.\n",
      "The text to be tokenized is a None type. Defaulting to blank string.\n",
      "The text to be tokenized is a None type. Defaulting to blank string.\n",
      "The text to be tokenized is a None type. Defaulting to blank string.\n",
      "The text to be tokenized is a None type. Defaulting to blank string.\n"
     ]
    }
   ],
   "source": [
    "# print(type(test_data['description']))\n",
    "# print(type(test_data.at['description',4729]))\n",
    "# test_data['description'].dropna(inplace=True)\n",
    "# train_data['description'].dropna(inplace=True)\n",
    "# print(train_data['description'].iloc[4729])\n",
    "# test_tokenized = test_data.apply(lambda r: w2v_tokenize_text(r['description']), axis=1).values\n",
    "# train_tokenized = train_data.apply(lambda r: w2v_tokenize_text(r['description']), axis=1).values\n",
    "# test_tokenized = df.description.apply(nltk.word_tokenize)\n",
    "\n",
    "# train_data.dropna(subset=['description'])\n",
    "# test_data.dropna(subset=['description'])\n",
    "test_tokenized = test_data[\"description\"].fillna(\"\").map(w2v_tokenize_text)\n",
    "train_tokenized = train_data[\"description\"].fillna(\"\").map(w2v_tokenize_text)\n",
    "\n",
    "\n",
    "# df.sample(5)\n",
    "# na.omit(dat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\gx\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:9: DeprecationWarning: Call to deprecated `syn0norm` (Attribute will be removed in 4.0.0, use self.vectors_norm instead).\n",
      "  if __name__ == '__main__':\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 11.7 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "X_train_word_average = word_averaging_list(model,train_tokenized)\n",
    "X_test_word_average = word_averaging_list(model,test_tokenized)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\gx\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:14: DeprecationWarning: Call to deprecated `syn0norm` (Attribute will be removed in 4.0.0, use self.vectors_norm instead).\n",
      "  \n"
     ]
    }
   ],
   "source": [
    "\n",
    "def word_append(wv, words):\n",
    "    all_words, mean = set(), []\n",
    "#     count = 0\n",
    "    for word in words:\n",
    "#         count+=1\n",
    "#         if count>69:\n",
    "#             break\n",
    "        if len(mean)>99:\n",
    "            break\n",
    "        if isinstance(word, np.ndarray):\n",
    "            mean.append(word)\n",
    "        elif word in wv.vocab:\n",
    "#             print(wv.vocab[word].index)\n",
    "            mean.append(wv.syn0norm[wv.vocab[word].index])\n",
    "#             print(wv.syn0norm[wv.vocab[word].index].shape)\n",
    "            all_words.add(wv.vocab[word].index)\n",
    "#     print(\"=============================\")\n",
    "#     print(len(mean))\n",
    "#     if count<70:\n",
    "    lengt = len(mean)\n",
    "    if lengt<100:\n",
    "        for i in range(100-lengt):\n",
    "            x = np.zeros((25,))\n",
    "            mean.append(x)\n",
    "#     print(len(mean))\n",
    "#     print(count)\n",
    "#     print(mean.shape)\n",
    "    if not mean:\n",
    "#         logging.warning(\"cannot compute similarity with no input %s\", words)\n",
    "        # FIXME: remove these examples in pre-processing\n",
    "        print(\"not mean\")\n",
    "        return np.zeros(25,)\n",
    "\n",
    "    mean = gensim.matutils.unitvec(np.array(mean)).astype(np.float32)\n",
    "#     mean = np.array(mean)\n",
    "#     print(mean.shape)\n",
    "#     print(mean)\n",
    "#     cccount+=1\n",
    "#     print(cccount)\n",
    "    \n",
    "    return mean\n",
    "\n",
    "def  word_list(wv, text_list):\n",
    "    ret = []\n",
    "#     i = 0\n",
    "    for review in text_list:\n",
    "#         i+=1\n",
    "#         print(i,\":\")\n",
    "        x = word_append(wv, review)\n",
    "#         print(x.size())\n",
    "        ret.append(x)\n",
    "    return ret\n",
    "#     return np.vstack([word_append(wv, review) for review in text_list ])\n",
    "\n",
    "X_train_word = word_list(model,train_tokenized)\n",
    "X_test_word = word_list(model,test_tokenized)\n",
    "\n",
    "# X_train_word = word_list(model,['SHANTELL', \"'S\", 'CHANNEL', 'https'])\n",
    "\n",
    "# print(train_tokenized[8])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "40949\n",
      "100.0\n",
      "(36854, 100, 25)\n",
      "(4095, 100, 25)\n",
      "(4095, 100, 25)\n",
      "(40949, 100, 25)\n"
     ]
    }
   ],
   "source": [
    "ave = 0\n",
    "count=0\n",
    "for i in X_train_word:\n",
    "    ave += i.shape[0]\n",
    "    count+=1\n",
    "for i in X_test_word:\n",
    "    ave += i.shape[0]\n",
    "    count+=1\n",
    "print(count)\n",
    "print(ave/count)\n",
    "\n",
    "print(np.array(X_train_word).shape)\n",
    "print(np.array(X_test_word).shape)\n",
    "\n",
    "X_train_words = np.array(X_train_word).reshape(-1,100,25)\n",
    "X_test_words = np.array(X_test_word).reshape(-1,100,25)\n",
    "print(X_test_words.shape)\n",
    "features = np.concatenate((X_train_words,X_test_words),axis=0)\n",
    "print(features.shape)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train set :  (32759, 100, 25) (32759,)\n",
      "Validation set :  (4095, 100, 25) (4095,)\n",
      "Test set :  (4095, 100, 25) (4095,)\n"
     ]
    }
   ],
   "source": [
    "split_frac = 0.8\n",
    "\n",
    "## split data into training, validation, and test data (features and labels, x and y)\n",
    "encoded_labels = np.array(y_train)\n",
    "\n",
    "\n",
    "length = len(features)\n",
    "split = int(split_frac*length)\n",
    "\n",
    "train_x,valid_x = features[:split],features[split:]\n",
    "train_y,valid_y = encoded_labels[:split],encoded_labels[split:]\n",
    "\n",
    "test_x,test_y = valid_x[len(valid_x)//2:],valid_y[len(valid_y)//2:]\n",
    "valid_x,valid_y = valid_x[:len(valid_x)//2],valid_y[:len(valid_y)//2]\n",
    "## print out the shapes of your resultant feature data\n",
    "print('Train set : ',train_x.shape,train_y.shape)\n",
    "print('Validation set : ',valid_x.shape,valid_y.shape)\n",
    "print('Test set : ',test_x.shape,test_y.shape)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import torch\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "\n",
    "# create Tensor datasets\n",
    "train_data = TensorDataset(torch.from_numpy(train_x), torch.from_numpy(train_y))\n",
    "valid_data = TensorDataset(torch.from_numpy(valid_x), torch.from_numpy(valid_y))\n",
    "test_data = TensorDataset(torch.from_numpy(test_x), torch.from_numpy(test_y))\n",
    "\n",
    "# dataloaders\n",
    "batch_size = 50\n",
    "\n",
    "# make sure to SHUFFLE your data\n",
    "train_loader = DataLoader(train_data, shuffle=True, batch_size=batch_size,drop_last=True)\n",
    "valid_loader = DataLoader(valid_data, shuffle=True, batch_size=batch_size,drop_last=True)\n",
    "test_loader = DataLoader(test_data, shuffle=True, batch_size=batch_size,drop_last=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample input size:  torch.Size([50, 100, 25])\n",
      "Sample input: \n",
      " tensor([[[ 4.0865e-03,  2.8314e-04,  1.6661e-03,  ...,  3.1024e-03,\n",
      "           3.6803e-03, -3.8615e-03],\n",
      "         [-1.0819e-02,  8.0137e-03,  9.8439e-03,  ...,  1.0166e-02,\n",
      "           7.7864e-03, -1.0328e-02],\n",
      "         [ 8.5514e-03,  1.4799e-02, -2.9018e-02,  ...,  6.8478e-03,\n",
      "          -4.0938e-03, -1.1484e-02],\n",
      "         ...,\n",
      "         [ 1.9571e-02, -1.5400e-02, -1.5154e-02,  ..., -1.8230e-03,\n",
      "           1.0738e-02,  2.7965e-03],\n",
      "         [ 1.9571e-02, -1.5400e-02, -1.5154e-02,  ..., -1.8230e-03,\n",
      "           1.0738e-02,  2.7965e-03],\n",
      "         [ 1.9571e-02, -1.5400e-02, -1.5154e-02,  ..., -1.8230e-03,\n",
      "           1.0738e-02,  2.7965e-03]],\n",
      "\n",
      "        [[-1.3251e-02,  1.3172e-02,  6.0367e-03,  ..., -5.9420e-03,\n",
      "           1.0465e-02, -2.0027e-02],\n",
      "         [ 5.0139e-03,  5.4120e-03, -1.7084e-03,  ...,  9.8442e-03,\n",
      "           3.8223e-03, -1.3325e-02],\n",
      "         [-9.9126e-03,  7.7583e-03, -2.0886e-03,  ..., -6.3381e-04,\n",
      "           1.7982e-02, -1.8397e-02],\n",
      "         ...,\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
      "           0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
      "           0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
      "           0.0000e+00,  0.0000e+00]],\n",
      "\n",
      "        [[-7.7279e-03, -6.0425e-03,  1.5359e-02,  ..., -3.3858e-03,\n",
      "          -3.3984e-03, -1.1072e-02],\n",
      "         [-2.0554e-02,  3.3235e-02,  3.4456e-02,  ..., -1.4141e-02,\n",
      "          -2.0406e-02,  5.1799e-03],\n",
      "         [-2.9891e-03, -1.0872e-02,  1.2944e-02,  ..., -1.0533e-02,\n",
      "          -1.8565e-02, -2.2181e-02],\n",
      "         ...,\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
      "           0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
      "           0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
      "           0.0000e+00,  0.0000e+00]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[-2.8668e-04,  5.6941e-04,  6.0547e-03,  ...,  5.2962e-03,\n",
      "          -2.3756e-02, -8.7997e-03],\n",
      "         [-5.8191e-03,  1.4203e-02,  7.0326e-03,  ..., -3.2629e-03,\n",
      "          -1.9380e-02, -1.3962e-02],\n",
      "         [ 2.2860e-02, -2.4143e-02,  5.6058e-02,  ...,  1.7393e-02,\n",
      "          -2.6900e-02,  3.4243e-02],\n",
      "         ...,\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
      "           0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
      "           0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
      "           0.0000e+00,  0.0000e+00]],\n",
      "\n",
      "        [[-3.5187e-02,  2.9149e-02,  9.3226e-03,  ..., -2.6723e-02,\n",
      "           4.9804e-03,  7.0663e-03],\n",
      "         [ 2.6608e-03,  1.8313e-02,  5.3346e-04,  ...,  6.6579e-03,\n",
      "           8.8293e-03, -9.2199e-03],\n",
      "         [ 3.9645e-03,  7.0255e-03, -3.1604e-05,  ..., -3.2954e-03,\n",
      "          -1.6480e-02, -2.2192e-02],\n",
      "         ...,\n",
      "         [-1.4971e-04,  2.9736e-04,  3.1620e-03,  ...,  2.7659e-03,\n",
      "          -1.2406e-02, -4.5955e-03],\n",
      "         [ 1.5464e-02, -3.6947e-03, -6.5018e-03,  ...,  1.0336e-02,\n",
      "          -2.6129e-02,  2.8858e-03],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
      "           0.0000e+00,  0.0000e+00]],\n",
      "\n",
      "        [[-4.9051e-03,  1.1972e-02,  5.9279e-03,  ..., -2.7504e-03,\n",
      "          -1.6336e-02, -1.1769e-02],\n",
      "         [ 6.4417e-03, -1.9769e-02, -2.4385e-02,  ..., -1.6728e-02,\n",
      "          -4.4473e-02, -2.6590e-02],\n",
      "         [-1.4519e-03,  1.9862e-02, -1.5183e-03,  ..., -4.3965e-03,\n",
      "           1.0732e-02, -1.1300e-02],\n",
      "         ...,\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
      "           0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
      "           0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
      "           0.0000e+00,  0.0000e+00]]])\n",
      "\n",
      "Sample label size:  torch.Size([50])\n",
      "Sample label: \n",
      " tensor([1, 2, 1, 0, 1, 0, 2, 0, 0, 0, 2, 2, 1, 0, 0, 0, 0, 0, 0, 0, 1, 2, 2, 0,\n",
      "        1, 2, 2, 1, 1, 2, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0,\n",
      "        1, 0])\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# obtain one batch of training data\n",
    "dataiter = iter(train_loader)\n",
    "sample_x, sample_y = dataiter.next()\n",
    "\n",
    "print('Sample input size: ', sample_x.size()) # batch_size, seq_length\n",
    "print('Sample input: \\n', sample_x)\n",
    "print()\n",
    "print('Sample label size: ', sample_y.size()) # batch_size\n",
    "print('Sample label: \\n', sample_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No GPU available, training on CPU.\n"
     ]
    }
   ],
   "source": [
    "# First checking if GPU is available\n",
    "train_on_gpu=torch.cuda.is_available()\n",
    "\n",
    "if(train_on_gpu):\n",
    "    print('Training on GPU.')\n",
    "else:\n",
    "    print('No GPU available, training on CPU.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "class SentimentRNN(nn.Module):\n",
    "    \"\"\"\n",
    "    The RNN model that will be used to perform Sentiment analysis.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, vocab_size, output_size, embedding_dim, hidden_dim, n_layers, drop_prob=0.7):\n",
    "        \"\"\"\n",
    "        Initialize the model by setting up the layers.\n",
    "        \"\"\"\n",
    "        super(SentimentRNN, self).__init__()\n",
    "\n",
    "        self.output_size = output_size\n",
    "        self.n_layers = n_layers\n",
    "        self.hidden_dim = hidden_dim\n",
    "        \n",
    "        # define all layers\n",
    "#         self.embed = nn.Embedding(vocab_size,embedding_dim)\n",
    "        self.lstm = nn.LSTM(embedding_dim,hidden_dim,n_layers,dropout=drop_prob,batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_dim,output_size)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "        self.drp = nn.Dropout(p=0.7)\n",
    "\n",
    "    def forward(self, x, hidden):\n",
    "        \"\"\"\n",
    "        Perform a forward pass of our model on some input and hidden state.\n",
    "        \"\"\"\n",
    "        batch_size=x.shape[0]\n",
    "        \n",
    "#         x = self.embed(x)\n",
    "       \n",
    "        x,hidden = self.lstm(x,hidden)\n",
    "        \n",
    "        x = x.reshape(-1,self.hidden_dim)\n",
    "        \n",
    "        x = self.drp(x)\n",
    "        \n",
    "        x = self.fc(x)\n",
    "        \n",
    "        sig_out = self.sigmoid(x)\n",
    "        \n",
    "        # return last sigmoid output and hidden state\n",
    "        sig_out = sig_out.reshape(batch_size,-1)\n",
    "        sig_out = sig_out[:,-1]\n",
    "        \n",
    "        return sig_out, hidden\n",
    "    \n",
    "    \n",
    "    def init_hidden(self, batch_size):\n",
    "        ''' Initializes hidden state '''\n",
    "        # Create two new tensors with sizes n_layers x batch_size x hidden_dim,\n",
    "        # initialized to zero, for hidden state and cell state of LSTM\n",
    "        weight = next(self.parameters()).data\n",
    "\n",
    "        if (train_on_gpu):\n",
    "            hidden = (weight.new(self.n_layers, batch_size, self.hidden_dim).zero_().cuda(),\n",
    "                  weight.new(self.n_layers, batch_size, self.hidden_dim).zero_().cuda())\n",
    "        else:\n",
    "            hidden = (weight.new(self.n_layers, batch_size, self.hidden_dim).zero_(),\n",
    "                      weight.new(self.n_layers, batch_size, self.hidden_dim).zero_())\n",
    "\n",
    "        \n",
    "        return hidden"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SentimentRNN(\n",
      "  (lstm): LSTM(25, 100, num_layers=2, batch_first=True, dropout=0.7)\n",
      "  (fc): Linear(in_features=100, out_features=1, bias=True)\n",
      "  (sigmoid): Sigmoid()\n",
      "  (drp): Dropout(p=0.7, inplace=False)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# Instantiate the model w/ hyperparams\n",
    "vocab_size = 1193514+1#???\n",
    "output_size = 1\n",
    "embedding_dim = 25\n",
    "hidden_dim = 100\n",
    "n_layers = 2\n",
    "\n",
    "net = None\n",
    "net = SentimentRNN(vocab_size, output_size, embedding_dim, hidden_dim, n_layers)\n",
    "\n",
    "print(net)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loss and optimization functions\n",
    "lr=0.001\n",
    "\n",
    "criterion = nn.BCELoss()\n",
    "optimizer = torch.optim.Adam(net.parameters(), lr=lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running on cpu\n",
      "e: 0\n",
      "torch.Size([2, 50, 100])\n",
      "\n",
      "Batch : 655/655hahhahhahahahha\n",
      "\n",
      "Epoch: 1/10... Step: 655... Loss: 0.716355... Val Loss: 0.545677\n",
      "\t\t...Saving Model...\n",
      "e: 1\n",
      "torch.Size([2, 50, 100])\n",
      "\n",
      "Batch : 655/655hahhahhahahahha\n",
      "\n",
      "Epoch: 2/10... Step: 1310... Loss: 0.687574... Val Loss: 0.554471\n",
      "e: 2\n",
      "torch.Size([2, 50, 100])\n",
      "\n",
      "Batch : 655/655hahhahhahahahha\n",
      "\n",
      "Epoch: 3/10... Step: 1965... Loss: 0.672308... Val Loss: 0.538521\n",
      "\t\t...Saving Model...\n",
      "e: 3\n",
      "torch.Size([2, 50, 100])\n",
      "\n",
      "Batch : 655/655hahhahhahahahha\n",
      "\n",
      "Epoch: 4/10... Step: 2620... Loss: 0.672880... Val Loss: 0.535200\n",
      "\t\t...Saving Model...\n",
      "e: 4\n",
      "torch.Size([2, 50, 100])\n",
      "\n",
      "Batch : 655/655hahhahhahahahha\n",
      "\n",
      "Epoch: 5/10... Step: 3275... Loss: 0.630923... Val Loss: 0.543875\n",
      "e: 5\n",
      "torch.Size([2, 50, 100])\n",
      "\n",
      "Batch : 655/655hahhahhahahahha\n",
      "\n",
      "Epoch: 6/10... Step: 3930... Loss: 0.643506... Val Loss: 0.538534\n",
      "e: 6\n",
      "torch.Size([2, 50, 100])\n",
      "\n",
      "Batch : 655/655hahhahhahahahha\n",
      "\n",
      "Epoch: 7/10... Step: 4585... Loss: 0.691135... Val Loss: 0.531363\n",
      "\t\t...Saving Model...\n",
      "e: 7\n",
      "torch.Size([2, 50, 100])\n",
      "\n",
      "Batch : 655/655hahhahhahahahha\n",
      "\n",
      "Epoch: 8/10... Step: 5240... Loss: 0.661084... Val Loss: 0.553781\n",
      "e: 8\n",
      "torch.Size([2, 50, 100])\n",
      "\n",
      "Batch : 655/655hahhahhahahahha\n",
      "\n",
      "Epoch: 9/10... Step: 5895... Loss: 0.672497... Val Loss: 0.544007\n",
      "e: 9\n",
      "torch.Size([2, 50, 100])\n",
      "\n",
      "Batch : 655/655hahhahhahahahha\n",
      "\n",
      "Epoch: 10/10... Step: 6550... Loss: 0.719155... Val Loss: 0.529075\n",
      "\t\t...Saving Model...\n"
     ]
    }
   ],
   "source": [
    "# training params\n",
    "validLoss,trainLoss = [],[]\n",
    "epochs = 10 # 3-4 is approx where I noticed the validation loss stop decreasing\n",
    "\n",
    "counter = 0\n",
    "print_every = 100\n",
    "clip=5 # gradient clipping\n",
    "minValidLoss = np.inf #use for saving model whenever valid loss becomes less than min valid loss\n",
    "\n",
    "# move model to GPU, if available\n",
    "device = 'cuda' if(torch.cuda.is_available()) else 'cpu'\n",
    "net.to(device)\n",
    "print(\"Running on\",device)\n",
    "\n",
    "net.train()\n",
    "# train for some number of epochs\n",
    "for e in range(epochs):\n",
    "    # initialize hidden state\n",
    "    print(\"e:\",e)\n",
    "    h = net.init_hidden(batch_size)\n",
    "    print(h[0].shape)\n",
    "    # batch loop\n",
    "    print()\n",
    "    for batch,(inputs, labels) in enumerate(train_loader,1):\n",
    "        print(f'\\rBatch : {batch}/{len(train_loader)}',end='')\n",
    "        counter += 1\n",
    "        \n",
    "        if(train_on_gpu):\n",
    "            inputs, labels = inputs.cuda().long(), labels.cuda().long()\n",
    "\n",
    "        # Creating new variables for the hidden state, otherwise\n",
    "        # we'd backprop through the entire training history\n",
    "        h = tuple([each.data for each in h])\n",
    "        \n",
    "\n",
    "        # zero accumulated gradients\n",
    "        net.zero_grad()\n",
    "        \n",
    "#         htemp = tuple([each.data for each in h])\n",
    "#         print(inputs[2].shape)\n",
    "#         print(\"======================\")\n",
    "#         print(h[1].shape)\n",
    "        # get the output from the model\n",
    "        output, h = net(inputs, h)\n",
    "#         print(h[1].shape)\n",
    "        \n",
    "\n",
    "        # calculate the loss and perform backprop\n",
    "        loss = criterion(output.squeeze(), labels.float())\n",
    "        loss.backward()\n",
    "        # `clip_grad_norm` helps prevent the exploding gradient problem in RNNs / LSTMs.\n",
    "        nn.utils.clip_grad_norm_(net.parameters(), clip)\n",
    "        optimizer.step()\n",
    "#         print(\"ok\")\n",
    "\n",
    "        # loss stats\n",
    "    else:\n",
    "        # Get validation loss\n",
    "        print(\"hahhahhahahahha\")\n",
    "        print()\n",
    "        val_h = net.init_hidden(batch_size)\n",
    "        val_losses = []\n",
    "        net.eval()\n",
    "        for inputs, labels in valid_loader:\n",
    "\n",
    "            # Creating new variables for the hidden state, otherwise\n",
    "            # we'd backprop through the entire training history\n",
    "            val_h = tuple([each.data for each in val_h])\n",
    "\n",
    "            if(train_on_gpu):\n",
    "                inputs, labels = inputs.cuda().long(), labels.cuda().long()\n",
    "\n",
    "            output, val_h = net(inputs, val_h)\n",
    "            val_loss = criterion(output.squeeze(), labels.float())\n",
    "\n",
    "            val_losses.append(val_loss.item())\n",
    "\n",
    "        net.train()\n",
    "        print(\"Epoch: {}/{}...\".format(e+1, epochs),\n",
    "              \"Step: {}...\".format(counter),\n",
    "              \"Loss: {:.6f}...\".format(loss.item()),\n",
    "              \"Val Loss: {:.6f}\".format(np.mean(val_losses)))\n",
    "        \n",
    "        if(np.mean(val_losses)<minValidLoss):\n",
    "          print('\\t\\t...Saving Model...')\n",
    "          torch.save(net.state_dict(),'LSTM_Model.pt')\n",
    "          minValidLoss = np.mean(val_losses)\n",
    "          \n",
    "        trainLoss.append(loss.item())\n",
    "        validLoss.append(np.mean(val_losses))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYAAAAD5CAYAAAAuneICAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4xLjEsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy8QZhcZAAAgAElEQVR4nO3dd3gU5fbA8e9JQgDpJXCRIqAgUgQkIIjSwQhKufQiVVHUH1e8ouC13ItdL2C5WJAAht4RFIkgipSAhF4UKVICKL1JCUnO749ZZImBLJBkNtnzeZ59svvOOzNnN7tzZt555x1RVYwxxgSeILcDMMYY4w5LAMYYE6AsARhjTICyBGCMMQHKEoAxxgQoSwDGGBOgQnypJCIRwPtAMDBKVd9KNn040NDz8iagiKrmF5FqwMdAXiAReF1Vp3jmGQvUB0545uupquuuFkfhwoW1dOnSvoRsjDHGY/Xq1YdVNSx5eaoJQESCgRFAUyAOWCUic1R1y8U6qjrAq/7/AdU9L88A3VV1m4jcDKwWkWhVPe6ZPlBVp/v6JkqXLk1sbKyv1Y0xxgAisjulcl+agGoB21V1p6rGA5OBVlep3xmYBKCqv6jqNs/z/cBB4C9ZyBhjTMbzJQEUB/Z6vY7zlP2FiNwClAEWpTCtFhAK7PAqfl1ENojIcBHJ7nPUxhhjbpgvCUBSKLvS+BGdgOmqmnjZAkSKAeOAXqqa5CkeDFQAagIFgedTXLlIXxGJFZHYQ4cO+RCuMcYYX/iSAOKAkl6vSwD7r1C3E57mn4tEJC/wFfCiqq64WK6qB9RxHhiD09T0F6o6UlXDVTU8LMxaj4wxJq34kgBWAeVEpIyIhOJs5OckryQitwMFgBivslBgFhClqtOS1S/m+StAa2DT9b4JY4wx1y7VXkCqmiAiTwHRON1AR6vqZhEZAsSq6sVk0BmYrJcPL9oBqAcUEpGenrKL3T0niEgYThPTOuDxNHlHxhhjfCKZaTjo8PBwtW6gxhhzbURktaqGJy+3K4GNMcaPHfzjIM9EP8Pp+NNpvmxLAMYY46dUlUfnPsqIVSPYfTzFa7luiE9DQRhjjMl4o9eOZs7WOQxtNpRKRSql+fID4ghg9NrRDIsZ5nYYxhjjsx1Hd/CP+f+gYemGPF376XRZR5Y/AlBVFuxcwORNk9l7Yi9D7x9KkARE3jPGZFIJSQk8POthQoJC+Lz15+m2zcryCUBEGN9mPEVzFeW9le8RdyqOcW3GkSMkh9uhGWNMit5Z9g4xcTFM+PsESuYrmfoM1ynLJwCA4KBg3ot4j1L5SvHPb/7J76d/Z3an2RTMWdDt0Iwx5jKr96/mle9foWOljnSu3Dld1xVQbSHP1HmGyW0ns3LfSu4dfW+6nFU3xpjrdfbCWbrN6kbRXEX5qMVHOAMlpJ+ASgAAHSt3JLpbNPtP7adOZB3W/XbVe9AYY0yGeX7h8/x8+GfGtBqTIS0UAZcAABqUbsCy3ssICQrhvjH3sWDHArdDMsYEuG92fMOHP35I/1r9aXpr0wxZZ0AmAIBKRSoR0yeGsgXK0nxic6LWR7kdkjEmQB09e5ReX/TijsJ38FaTt1KfIY0EbAIAKJ63OD/0/IH6t9Snx+wevLHkDTLT2EjGmMxPVen3VT8O/nGQ8X8fT85sOTNs3QGdAADy5cjHvK7z6FqlK/9a9C+e+OoJEpIS3A7LGBMgJm6cyNTNUxnSYAh3FbsrQ9cdEN1AUxMaHEpUmyhK5i3JW8veYt+pfUxqO4lcobncDs0Yk4XtObGHJ+c9Sd2SdXmu7nMZvv6APwK4KEiCeLPJm4xoPoKvtn1Fo6hGHPrDbkFpjEkfSZpEj9k9SNREotpEERwUnOExWAJI5omaTzCzw0w2/L6Be0bfw46jO1KfyRhjrtHwmOF8v+t73o94n7IFyroSgyWAFLSq0IpF3Rdx7Owx6kTW4cd9P7odkjEmC9n4+0ZeWPQCrSu0ple1Xq7F4VMCEJEIEdkqIttFZFAK04eLyDrP4xcROe41rYeIbPM8eniV1xCRjZ5lfiDpfcnbNapTsg7L+ywnd2huGoxtwJe/fOl2SMaYLOB8wnm6zepG/hz5GfngyHS/2vdqUk0AIhIMjAAeACoCnUWkoncdVR2gqtVUtRrwITDTM29B4BXgbqAW8IqIFPDM9jHQFyjneUSkyTtKQ+ULlSemTwwVwyrSanIrRq4e6XZIxphM7qXvXmLD7xsY3XI0YbnCXI3FlyOAWsB2Vd2pqvHAZKDVVep3BiZ5nt8PLFDVo6p6DFgARIhIMSCvqsZ4biIfBbS+7neRjormLsr3Pb8n4rYIHvvyMV5a9JJdK2CMuS6Ldy3mv8v/y2M1HqNF+RZuh+NTAigO7PV6Hecp+wsRuQUoAyxKZd7inuepLtMf5A7NzRedvqBP9T68tuQ1en3RiwuJF9wOyxiTiZw4d4Lus7tza8Fb+W+z/7odDuDbdQApNVBdaRe4EzBdVRNTmdfnZYpIX5ymIkqVKnX1SNNRSFAInz30GSXzluTfi//N/lP7mdFhBnmy53EtJmNM5tF/fn/2ndzH0t5LyR2a2+1wAN+OAOIA7zsSlAD2X6FuJy41/1xt3jjP81SXqaojVTVcVcPDwtxtLxMRXmnwCpEtI1n06yLqja3HgVMHXI3JGOP/pm+ZTtT6KP5137+oXaK22+H8yZcEsAooJyJlRCQUZyM/J3klEbkdKADEeBVHA81EpIDn5G8zIFpVDwCnRKS2p/dPd+CLG3wvGaZ39d7M7TyXbUe2USeyDj8d+sntkIwxfmr/qf089uVj1Ly5Ji/We9HtcC6TagJQ1QTgKZyN+U/AVFXdLCJDRKSlV9XOwGT1OkOqqkeBV3GSyCpgiKcMoB8wCtgO7AC+ToP3k2EeKPcAi3su5lzCOeqOrsvSPUvdDskY42dUld5f9ObshbOMazOObMHZ3A7pMpKZerSEh4drbGys22Fc5tdjvxIxIYLdx3cz4e8TaFuxrdshGWP8xIgfR/DU108xovkInqj5hGtxiMhqVQ1PXm5XAt+gMgXKsKz3Mu4qdhftp7Xn/RXvux2SMcYP/Hz4ZwYuGEjEbRH0C+/ndjgpsgSQBgrfVJhvu39L6wqteTr6aZ795lmSNMntsIwxLrmQeIGHZz1Mzmw5Gd1ytKtX+16NJYA0kjNbTqa1n8ZTNZ9iaMxQus7syvmE826HZYxxwas/vErs/lhGPjiSYnmKuR3OFdn9ANJQcFAwHzzwASXzleT5hc9z4NQBZneaTf4c+d0OzRif/Hz4Z7IHZ6dMgTJuh5JprYhbwetLXqdH1R5+f07QjgDSmIjwXN3nmPD3CSzfu5x7R9/L3hN7U5/RGJftO7mPOpF1qPZpNX7Y/YPb4WRKp+NP021mN0rmLcn7Ef5/PtASQDrpUqUL87vNZ+/JvdSOrM2G3ze4HZIxV6Sq9JnTh/jEeIrlLsb94+9n7ta5boeV6fwz+p/sPLaTqDZR5MuRz+1wUmUJIB01KtOIpb2WIgj3jbmPRb8uSn0mY1zw6epPid4RzbtN32Vp76VUKVKFNlPaELU+yu3QMo25W+cycs1IBt4zkHq31HM7HJ9YAkhnVYpWIaZPDCXzliRifATvLHuH0/Gn3Q7LmD/tOLqDZ795lqZlm9IvvN+fvdoalG5Aj9k9eG/Fe26H6PcO/nGQR+Y+QtWiVRnScIjb4fjMEkAGKJmvJEt7L6XZrc14fuHzlBpeile+e4XDZw67HZoJcIlJifT8oichQSFEtoz8s7tinux5+KrLV7S9oy0DogfYMOhXoar0nduX4+eOM/7v48kekt3tkHxmCSCD5M+Rny+7fElMnxjql67PkB+GUGp4Kf7x9T/Yc2KP2+GZADV8xXCW7ln6Z+81b9lDsjOl3RQeqf4Iry15jSfnPUliUuIVlhS4Rq8dzRdbv+DNxm9SuUhlt8O5JjYUhEt+OvQT7yx/h/EbxgPQtUpXnqv7HBXDKqYypzFpY/PBzdw18i6al2vOzA4zr3ixkqoy+NvBvL3sbTpW6khUmyhCg0MzOFr/tOPoDqp+UpVaxWuxsPtCgsQ/96ltKAg/c0fYHYxpNYYd/XfwZM0nmbZlGpU+qkTrya1ZEbfC7fBMFnch8QLdZ3cnX/Z8fPrgp1e9UlVEeKvJW7zT5B2mbJ5Cy0kt+SP+jwyM1j8lJCXQfXZ3QoJC+Lz153678b+azBdxFlMqXynei3iP3U/v5pX6r7BkzxLqRNahwdgGzN8+39pdTbp4fcnrrDmwhk8e/IQiuYr4NM/AugOJbBnJgp0LaDquKUfPHk19pizsnWXvsHzvcj5q8dFfms8yC2sC8jOn408zas0ohsYMJe5kHNX+Vo1BdQfRrmI7goOC3Q7PZAGx+2OpPao2nat0Zlybcdc8/6yfZtFpRifKFypPdLdobs5zczpE6d/WHFjD3aPupu0dbZnUdpLfjvVz0ZWagCwB+Kn4xHgmbJjA28veZuuRrdxa4FYG3jOQHtV6kCMkh9vhmUzq7IWz1BhZg5PnT7LpiU3XPUzJol8X0WpyK8JuCuObh7/htoK3pXGk/sv7M9zQbwMFcxZ0O6RU2TmATCY0OJRe1Xux5cktzOwwk4I5C/L4V49T5v0yvLPsHU6eP+l2iCYTenHRi/x0+CdGtxp9Q2NUNSrTiEXdF3Hy/EnuHX0v639bn4ZR+rdBCwfx0+GfGNNqTKbY+F+NJQA/FyRBtLmjDSsfWcmi7ouoUqTKn9cSvPDtC/x++ne3QzSZxOJdixm+Yjj9wvvR7NZmN7y8msVrsrT3UrIFZ6P+2PoBcVe8BTsW8MGPH9C/Vn+a3trU7XBumE9NQCISAbwPBAOjVPWtFOp0AP4NKLBeVbuISENguFe1CkAnVZ0tImOB+sAJz7SeqrruanEEUhPQ1azev5q3l73N9C3TCQ0OpXf13gy8Z6CN4Giu6NT5U1T9pCpBEsS6x9eROzR3mi17z4k9NBvXjN0ndjO9/XRalG+RZsv2J0fPHqXKx1XIlz0fq/uuJme2nG6H5LMrNQGhqld94Gz0dwBlgVBgPVAxWZ1ywFqggOd1kRSWUxA4CtzkeT0WaJfa+r0fNWrUUHPJ1sNb9dE5j2roq6Ea/J9g7TKji67/bb3bYRk/1HdOX5V/iy7ZvSRdln/w9EGt8WkNDf5PsI5fPz5d1uGmpKQk7TCtg4YMCdHV+1e7Hc41A2I1hW2qL01AtYDtqrpTVeOByUCrZHUeBUao6jFPUjmYwnLaAV+r6hkf1ml8UL5QeUY+NJJf//ErA2oPYM7WOVT9pCotJrZgye4lbodn/MTX275m5JqRPHvPs9xb6t50WUdYrjAW9VhEvVvq0W1WNz5c+WG6rMctEzdOZOrmqQxpMIS7it3ldjhpxpcEUBzwHtA+zlPmrTxQXkSWicgKT5NRcp2AScnKXheRDSIyXEQyzwAafubmPDfzbrN32fP0Hl5r+Bo/7vuRemPrce/oe/nyly/t9pQB7OjZo/SZ04dKYZXSfZCyvNnzMq/rPFpXaE3/+f359/f/zhLXsew5sYcn5z3JPSXv4bm6z7kdTpryJQGk1ME1+X81BKcZqAHQGRglIn92MRCRYkAVINprnsE45wRq4jQPPZ/iykX6ikisiMQeOnTIh3ADV4GcBfhXvX+x++nd/O+B/xF3Mo6HJj1E1U+qMn7DeC4kXnA7RJPBnpr3FIfOHCKqTVSGdB/OEZKDae2n0ataL/6z+D/0/7p/pt4BSdIkes7uSaImMq7NuCx3LY4vt4SMA7wvcysB7E+hzgpVvQD8KiJbcRLCKs/0DsAsz3QAVPWA5+l5ERkDPJvSylV1JDASnJPAPsQb8G7KdhNP1nqSvjX6MmXzFN5a+hYPz3qYl757iWfrPEuv6r24KdtNPi9PVUnURBKSEriQeIELSRf+fJ6QlHDdr3OH5qbl7S0JCbI7k6aHaZunMWnTJP7T4D8Z2mxxcWTRgjkLMjRmKEfOHuHz1p+TLThbhsWQVt5b8R7f7fqOyJaRlC1Q1u1w0lyqvYBEJAT4BWgM7MPZqHdR1c1edSKAzqraQ0QK45wQrqaqRzzTVwCDVfU7r3mKqeoBcS6hGw6cU9VBV4vFegFdnyRNYt62eby59E2W711OoZyFKJWv1DVtrNPLM7WfYej9Q9Nt+YHqt9O/UfmjypQpUIblvZe7svFVVd5e9jaDvx3MA7c9wPQO069px8Mt8YnxRG+PZuKmiczYMoMW5VtcdbC8zOBKvYBS3fVS1QQReQqn+SYYGK2qm0VkCM6Z5Tmeac1EZAuQCAz02viXxjmCWJxs0RNEJAyniWkd8Pj1vjlzdUESxIPlH+TB8g+ydM9SPo79mFPnTxESFEK24GzO36Bkf5OXX+frq9UZGjOUYSuGUadkHdpVbOf2x5RlqGd8+tPxp4lqHeXanreIMOjeQc5FjF8+TrNxzfiyy5c3dAFaeknSJJbsXsLEjROZtmUax84do1DOQjxy1yO81ui1TL3xvxobCsK4Jj4xnvpj67Pp4CZiH43l9sK3ux1SljBm7Rh6z+nNsGbDGFBngNvhADB9y3S6zOjCHWF3ML/rfIrlKeZ2SKgqa39by6SNk5i0aRL7Tu0jV7ZctK7Qmi5VutC0bNNM2WyVEhsLyPiluJNxVP+0OkVyFWHlIyvT9AKlQLT7+G6qfFyFu4rdxaIei/xqiOIFOxbQZkobiuYuyoKHF7jWpr7tyDYmbZrExI0T2XpkK9mCshFxWwRdqnThofIPkSs0lytxpSdLAMZvfbvzW5qNb0bHSh2Z8PcJWfZwO70laRJNxzXlx30/suHxDX55ZfjKuJU0n9ic0OBQvun2DVWKVsmQ9R44dYApm6cwceNEVu1fhSDUL12fLpW70LZi20w/pk9qbDA447cal23Mqw1fZdKmSfzvx/+5HU6mNeLHESz6dRHDmg3zy40/wN0l7uaHnj8QJEHUG1uP5XuXp9u6jp09RuSaSBpHNab4sOIMiB5Aoiby36b/Zc+APXzX4zserfFolt/4X40dARi/kKRJtJ7cmq+3f80PPX+gTsk6boeUqfxy5BeqfVKNBqUb8FWXr/z+KGrX8V00G9eMuJNxzOw4k4jbUrp29NqdvXCWL3/5kombJjJv2zziE+MpV7AcXap0oXPlzgF7nsmagIzfO37uODVG1uB8wnnWPLbG5ztVBbqEpATuHX0vvxz5hU1PbMo0N2j5/fTvREyIYNPBTYxrM45OlTtd13ISkhJYuHMhEzdOZNbPszgdf5piuYvRqXInulTpQo1iNfw+Iaa36+4GakxGyZ8jPzM6zKBOZB06z+hMdLdou0jMB+8ue5eV+1Yyqe2kTLPxByiauyjf9/ielpNb0mVGF46dPUa/mv18mldViYmL+XOMnkNnDpE/R346VepE5yqdqX9L/Sx31W56sF+X8SvV/laNj1t8TK8vevHydy/zRuM33A7Jr63/bT2vfP8K7Su2p2Oljm6Hc83y5cjH/K7z6Ti9I0/Me4LDZw7zYr0Xr7jHvungJiZunMikTZPYdXwXOUJy0PL2lnSp3IWI2yLIHmJDil0LSwDG7/Ss1pPle5fz5tI3ubv43bSqkHzwWQNwPuE83Wd3p2DOgnzU4qNM28yRM1tOZnacSZ85fXj5+5c5cvYIw+4f9mcX1l3HdzFp4yQmbprIpoObCJZgmt3ajCENhtC6QmvyZM/j8jvIvCwBGL/0wQMfsObAGnrM7kFs39iAuuesr/6z+D9s+H0DczvPpfBNhd0O54aEBIU4t1jMUZD3Vr7HkbNHqF28NhM3Tfyzp1DdknUZ0XwE7Su2JyxXmMsRZw12Etj4rV3Hd1FjZA1K5C1BTJ+YTDGOTEaJ2RvDvWPupWfVnkS2inQ7nDSjqryx5A1e/O5FAO4seiedK3emU+VOlM5f2t3gMjHrBWQypfnb59N8QnMervowY1uNzbTNHGnpzIUzVPukGucTz7Ox30byZs/rdkhp7rtfvyMsVxiVi1R2O5QswS4EM5lSxG0RvFz/ZaLWR/HZms/cDscvDFo4iG1HtzG21dgsufEHaFimoW38M4AlAOP3Xqr3Evffej//9/X/sWrfqtRnyMK+3fktH/74If1r9adhmYZuh2MyOUsAxu8FBwUz4e8T+Fvuv9FuWjuOnDnidkiuOHHuBL2+6EX5QuV5s8mbbodjsgBLACZTKHRTIaa3n85vp3+j68yuJCYluh1ShhsQPYB9p/YR1TrKToibNGEJwGQaNYvX5MMHPiR6RzSv/vCq2+FkqLlb5zJm3RgG1R3E3SXudjsck0VYAjCZyqN3PUqPqj0YsngIX2/72u1wMsThM4d5dO6jVC1alVcavOJ2OCYL8SkBiEiEiGwVke0ikuJ9e0Wkg4hsEZHNIjLRqzxRRNZ5HnO8ysuIyEoR2SYiU0Qk9MbfjsnqRISPWnxElaJV6DqzK7uO73I7pHSlqvT7qh9Hzx4lqk0UocH2MzFpJ9UEICLBwAjgAaAi0FlEKiarUw4YDNRV1UrA016Tz6pqNc+jpVf528BwVS0HHAP63NhbMYHipmw3MaPDDJI0iXZT23Eu4ZzbIaWbSZsmMX3LdIY0HMKdRe90OxyTxfhyBFAL2K6qO1U1HpgMJB+c5VFghKoeA1DVg1dboDhX8zQCpnuKPgdaX0vgJrDdVvA2otpEsfrAavp/3d/tcNLF/lP7eXLek9QuUZuB9wx0OxyTBfmSAIoDe71ex3nKvJUHyovIMhFZISLed3fIISKxnvKLG/lCwHFVTbjKMo25qpa3t2TwvYP5bM1njFk7xu1w0pSq0mdOH84nnCeqdZQNbWzShS+DwaV07X3y8SNCgHJAA6AEsEREKqvqcaCUqu4XkbLAIhHZCJz0YZnOykX6An0BSpUq5UO4JpAMaTiElftW8sS8J6herDrV/lbN7ZDSxGdrPmP+9vl8+MCHlCtUzu1wTBblyxFAHFDS63UJYH8Kdb5Q1Quq+iuwFSchoKr7PX93At8D1YHDQH4RCbnKMvHMN1JVw1U1PCzMRgA0lwsJCmFS20kUylmItlPbcuzsMbdDumE7j+3kmehnaFymMU/UfMLtcEwW5ksCWAWU8/TaCQU6AXOS1ZkNNAQQkcI4TUI7RaSAiGT3Kq8LbFFnBLrvgHae+XsAX9zomzGBqUiuIkxrP409J/bQY3YPkjTJ7ZCuW2JSIj1n9yQ4KJgxrcb8OSa+Mekh1W+Xp53+KSAa+AmYqqqbRWSIiFzs1RMNHBGRLTgb9oGqegS4A4gVkfWe8rdUdYtnnueBZ0RkO845gawzpq3JcHVK1mFYs2HM/WUuby19y+1wrtv7K99nyZ4lfBDxASXzlUx9BmNugA0HbbIMVaXrzK5M2TyF6G7RNCnbxO2QrsmWQ1u469O7uP+2+5ndcbYNfW3SjA0HbbI8EWHkQyOpULgCnWd0Zu+JvanP5CcuJF6g+6zu5Mmeh5EPjrSNv8kQlgBMlpI7NDczOszgXMI52k9rT3xivNshXVWSJvHtzm/5+9S/s/rAaj5p8QlFcxd1OywTICwBmCynQuEKjGk1hpX7VvJM9DNuh5OivSf28uriV7ntg9toMq4JS/cs5dWGr9K2Ylu3QzMBxG4Kb7KkdhXb8UztZxi2Yhh1StSh651d3Q6J+MR45mydQ+TaSL7Z8Q1JmkSjMo14rdFrtKnQhpzZcrodogkwlgBMlvVWk7dYtX8Vfb/sS9W/VXXtFoObD24mcm0k4zaM4/CZw5TIW4IX7n2BXtV7UbZAWVdiMgYsAZgsLFtwNqa0m0L1T6vTdmpbVj26KsPuoXvq/CmmbJ5C5NpIVsStIFtQNlre3pI+1fvQ7NZmNrSD8QuWAEyWVixPMaa2n0qjzxvR64teTG8/Pd162Kgqy/cuJ3JtJFM3T+WPC39QMawiQ5sN5eE7HyYsl13JbvyLJQCT5dW7pR5vN3mbZxc8y9CYoTx7z7NpuvzfT/9O1PooItdGsvXIVnKH5qZz5c70uasPdxe/27p0Gr9lCcAEhGfqPENMXAyDFg6i5s01qV+6/g0tLyEpgfnb5xO5NpIvf/mShKQE6pasy/N1n6d9pfbkDs2dRpEbk34sAZiAICKMbjWajQc30nF6R9Y+tpZieYpd83K2H93O6LWj+Xz95+w/tZ8iuYowoPYAelfvTYXCFdIhcmPSjyUAEzDyZs/LjA4zuHvU3XSY3oFF3ReRLThbqvOduXCGGVtmELk2ksW7FxMkQTxw2wOMaD6CFuVa+LQMY/yRJQATUCoXqcxnD31G15ldGbRwEEPvH5piPVVl9YHVRK6JZOKmiZw8f5JbC9zK641ep0fVHhTPa/cvMpmfJQATcLpU6ULM3hiGrRhG7RK1aV+p/Z/Tjp49yvgN44lcG8mG3zeQIyQH7Sq2o0/1PtS7pZ4Nz2yyFEsAJiANvX8osQdi6T2nN5WKVGLfyX1Ero1k1s+ziE+Mp0axGnzU/CM6V+lM/hz53Q7XmHRhCcAEpNDgUKa1n0b1T6tz58d3kqiJFMhRgMdqPEaf6n2o+reqbodoTLqzBGACVom8JZjZYSbvr3yfdhXb0bpCa3KE5HA7LGMyjCUAE9Duu+U+7rvlPrfDMMYVPp3REpEIEdkqIttFZNAV6nQQkS0isllEJnrKqolIjKdsg4h09Ko/VkR+FZF1nke1tHlLxhhjfJHqEYCIBAMjgKZAHLBKROZ43dsXESkHDAbqquoxESnimXQG6K6q20TkZmC1iESr6nHP9IGqOj0t35Axxhjf+HIEUAvYrqo7VTUemAy0SlbnUWCEqh4DUNWDnr+/qOo2z/P9wEHARsQyxhg/4EsCKA5431w1zlPmrTxQXkSWicgKEYlIvhARqQWEAju8il/3NA0NF5Hs1xi7McaYG+BLAkhpKENN9joEKAc0ADoDo0Tkz87TIlIMGAf0UtUkT/FgoAJQEygIPJ/iykX6ikisiMQeOnTIh3CNMcb4wpcEEAeU9HpdAtifQp0vVPWCqv4KbMVJCIhIXuAr4EVVXXFxBlU9oI7zwBicpqa/UNWRqhququFhYdZ6ZIwxacWXBLAKKCciZUQkFOgEzElWZzbQEEBECk8a2M0AABTcSURBVOM0Ce301J8FRKnqNO8ZPEcFiDNYemtg0428EWOMMdcm1V5AqpogIk8B0UAwMFpVN4vIECBWVed4pjUTkS1AIk7vniMi0g2oBxQSkZ6eRfZU1XXABBEJw2liWgc8ntZvzhhjzJWJavLmfP8VHh6usbGxbodhjDGZioisVtXw5OU2tKExxgQoSwDGGBOgLAEYY0yAsgRgjDEByhKAMcYEKEsAxhgToCwBGGNMgLIEYIwxAcoSgDHGBChLAMYYE6AsARhjTICyBGCMMQHKEoAxxgQoSwDGGBOgLAEYY0yAsgRgjDEByhKAMcYEKJ8SgIhEiMhWEdkuIoOuUKeDiGwRkc0iMtGrvIeIbPM8eniV1xCRjZ5lfuC5N7AxxpgMkuo9gUUkGBgBNAXigFUiMkdVt3jVKQcMBuqq6jERKeIpLwi8AoQDCqz2zHsM+BjoC6wA5gERwNdp+eaMMcZcmS9HALWA7aq6U1XjgclAq2R1HgVGeDbsqOpBT/n9wAJVPeqZtgCIEJFiQF5VjVHnpsRRQOs0eD/GGGN85EsCKA7s9Xod5ynzVh4oLyLLRGSFiESkMm9xz/OrLdMYY0w6SrUJCEipbV5TWE45oAFQAlgiIpWvMq8vy3RWLtIXp6mIUqVK+RCuMcYYX/hyBBAHlPR6XQLYn0KdL1T1gqr+CmzFSQhXmjfO8/xqywRAVUeqariqhoeFhfkQrjHGGF/4kgBWAeVEpIyIhAKdgDnJ6swGGgKISGGcJqGdQDTQTEQKiEgBoBkQraoHgFMiUtvT+6c78EWavCNjjDE+SbUJSFUTROQpnI15MDBaVTeLyBAgVlXncGlDvwVIBAaq6hEAEXkVJ4kADFHVo57n/YCxQE6c3j/WA8gYYzKQOJ1wMofw8HCNjY11OwxjjMlURGS1qoYnL7crgY0xJkBZAjDGmABlCcAYYwKUJQBjjAlQlgCMMSZAWQIwxpgAZQnAGGMClCUAY4wJUJYAjDEmQFkCMMaYAGUJwBhjApQlAGOMCVCWAIwxJkBZAjDGmABlCcAYYwKUJQBjjAlQlgCMMSZA+ZQARCRCRLaKyHYRGZTC9J4ickhE1nkej3jKG3qVrRORcyLS2jNtrIj86jWtWtq+NWOMMVeT6j2BRSQYGAE0BeKAVSIyR1W3JKs6RVWf8i5Q1e+Aap7lFAS2A994VRmoqtNvIH5jjDHXyZcjgFrAdlXdqarxwGSg1XWsqx3wtaqeuY55jTHGpDFfEkBxYK/X6zhPWXJtRWSDiEwXkZIpTO8ETEpW9rpnnuEikt23kI0xxqQFXxKApFCmyV7PBUqr6p3AQuDzyxYgUgyoAkR7FQ8GKgA1gYLA8ymuXKSviMSKSOyhQ4d8CNcYY4wvfEkAcYD3Hn0JYL93BVU9oqrnPS8/A2okW0YHYJaqXvCa54A6zgNjcJqa/kJVR6pquKqGh4WF+RCuMcYYX/iSAFYB5USkjIiE4jTlzPGu4NnDv6gl8FOyZXQmWfPPxXlERIDWwKZrC90YY8yNSLUXkKomiMhTOM03wcBoVd0sIkOAWFWdA/QXkZZAAnAU6HlxfhEpjXMEsTjZoieISBhOE9M64PEbfjfGGGN8JqrJm/P9V3h4uMbGxrodhjHGZCoislpVw5OX25XAxhgToCwBGGNMgLIEYIwxAcoSgDHGBChLAMYYE6AsARhjTICyBGCMMQHKEoAxxgQoSwDGGBOgLAEYY0yAsgRgjDEByhKAMcYEKEsAxhgToCwBGGNMgLIEYIwxAcoSQAZRhZ9/hmPH3I7EGGMclgDS0YEDEBUFDz8MxYrBHXdAqVIweDDY/e2NSdmFCzBqFEyaBMePux1N1uZTAhCRCBHZKiLbRWRQCtN7isghEVnneTziNS3Rq3yOV3kZEVkpIttEZIrnfsOZ2h9/wNdfwzPPQJUqcPPN0KMHzJ8PDRvCJ59Aixbw9ttQujQ8+yz89pvbURvjP7Zsgdq14dFHoUsXKFzY+e0MGwbbtrkdXRakqld94NwHeAdQFggF1gMVk9XpCfzvCvOfvkL5VKCT5/knQL/UYqlRo4b6k4QE1VWrVN94Q7VBA9XQUFVQzZ5dtXFj1bffVl2zRjUx8fL5tmxR7dZNNShINUcO1f79VePi3HkPxviDxETV4cOd306hQqrTpqkuW6Y6aJBqpUrO7wpUb79d9dlnVRcvVr1wwe2oMw+c+7f/dTucUqFevqGuA0R7vR4MDE5W55oSAM6N4A8DISmt40oPf0gAu3apfvaZavv2qgULXvpiVq3qfDGjo1XPnPFtWb/8otqrl2pwsJM8+vVT3b07feM3xt/s3q3asKHzO3rwQdUDB/5aZ8cO1fffV23SRDVbNqduwYKqXbuqTp6sevx4xsedmdxIAmgHjPJ6/XDyjb0nARwANgDTgZJe0xKAWGAF0NpTVhjY7lWnJLAptVjcSADHj6vOmqX6xBOq5cpd2uDffLNqjx6q48er/vbbja1j507Vvn2dL3a2bKqPPOJ84Y3JypKSVMeOVc2bVzV3btVRo5yy1Jw4oTp1qurDDztHC6AaEqLaqJFzFLF9e/rHntncSAJon0IC+DBZnUJAds/zx4FFXtNu9vwtC+wCbgXCUkgAG6+w/r6eBBJbqlSpdP+g4uNVly5VfeUV1XvucfbOQfWmm1SbN3e+YJs2+fZFvVa7d6s++aRzNBAc7CSYrVvTfj3GuO3gQdU2bZzf1r33Xv8OT0KC83t9/nnVihUv7aDdcYfqc8+pLlliTUWq6dwElKx+MHDiCtPGeo4o/KYJKCnJ2cj+73+qrVo5eyOgKqJas6bqCy+ofv+96vnzab7qK9q3T/Xpp53zA0FBql26qG7enHHrNyY9ffGFapEizo7OO+84G/G0sn276nvvOefgQkIuNRV166Y6ZUrgNhXdSAIIAXYCZbxOAldKVqeY1/M2wArP8wJeRwaFgW0XTyAD05KdBH4itVjSKgEcPux8GR55RPWWWy7tNZQu7TTFTJumeuRImqzqhhw44JxXuOkmJyF16KC6YYPbURlzfU6cUO3dW/88Z5be3+Xjx53febdul87XZcvmJIf33gusZtbrTgDOvDQHfsHpDfQvT9kQoKXn+ZvAZk9y+A6o4Cm/B9joKd8I9PFaZlngR2C7JxlkTy2O600A586pLlqkOniwao0azsYUVPPlcw5DP/pIddu29GnWSQsHDzqx58njxN2mjdO7yJjMYvFiZwcrKMj5Lp87l7Hrv3BB9YcfVAcOdJqHLu70VazoNB8tXZq2RyL+5koJQJxpmUN4eLjGxsZe83wNG8L330NIiNPHuGlT51GzplOWWRw9Cu+/7zxOnIAHH4SXXoJatdyOzJiUnTvnfEeHDoWyZeHzz6FuXbejgu3b4csvYe5c+OEHSEhwrjlo3hweegiaNYO8ed2OMu2IyGpVDf9LeSAkgHnznH9wgwZZ45964gR8+CEMH+4khfvvd35k/vDDymxOnoRff4Vbb4Xcud2OJmtZt865Cn7TJnjsMfjvf/3zMz5+HKKjnWQwb54zXEu2bM72omVL54K0ggXdjvLGBHQCyKpOnYKPPnL2rg4dgkaN4OWXoX59tyPzXxcuwMqVsGABLFzoPE9MdKaVKgUVK/71kS+fuzFnNgkJ8M478O9/Q6FCMHo0PPCA21H5JiEBli93ksHcubB1K+TKBY88AgMGwC23uB3h9bEEkIX98Qd8+qnzo/v9d7jvPicRNG4MIm5H5y5VZ3iBixv8xYvh9GkICnKaAJs0cYbt2LHDqbdlC/z0k9N0cVHx4iknhsy+V5getm+H7t0hJgbat4ePP3aSQGa1YYOzgzVxovNd6tgRBg6EatXcjuzaWAIIAGfPOoNovf027NvnnO94+WWIiAisRLBvH3z77aWN/sXxlsqXdzb4TZo454Xy5095/sRE2L37UkLwfvzxx6V6RYumnBjCwgLr8wZn4/jpp/DPf0JoKIwYAZ07Z53PYe9e59zbp586OxBNm8Jzz2WenSxLAAHk/HkYMwbefBP27IHwcOccwUMPZY4v67U6edLZs7+4wf/pJ6c8LOzSBr9JE6eJ50YkJTkbgpQSw8mTl+oVKpRyYihWLGt+/vv3Q58+zqCHTZs6TT4lSrgdVfo4ftwZ1PH9950di2rVnETQvr1/dyixBBCA4uNh3Dh44w3YuROqVnUSQZs2ThNIZnWldvycOZ3zHxc3+FWqZMz7VHU2gsmTwubNl9//IX/+lBNDiRKZNzFMmQL9+jlNZu++6zzPzN8tX50/DxMmOO/555+d0X0HDHASYa5cbkf3V5YAAlhCgtOG+dprzpC6FSs6PRyKF3eGrPZ+FCjgfxuji+34Cxc6G/2U2vGbNIE6dSB7drejvUQVDh68PCFcfO59P4jcuZ3/SZUqTrNdnTrOvSP8eUN69Cg89ZQzZn+tWs6ORvnybkeV8ZKSnO6k774LS5c654WeeAL+7/+gSBG3o7vEEoAhMRGmToX33nMSQUp3J8ue3UkEKSWHi4/ixdO/O9/FdvyFC53HgQNOeblyTjNDkyZOEitQIH3jSC+HDjlNVd7JYd06Z8MKTnflu+++lBDuvtt/Tjp/8w306uUkt5dfdm5w5M/NHxklJsZJBLNnO7+jHj2ccyLlyrkdmSUAk4KzZ50N6759ThNGSo99+y4/8XlRnjxXTg4XnxcrBjly+BbLxXb8i3v53u34jRs7G/3GjTNvNzxfqDqJecUKZ2OyYoXTCyUpyZlevryTDC4mhUqVMnbD+8cfTnv3Rx85RyjjxkGNGhm3/sxi61bnBjaff+40w7Zp43xud9/tXkyWAMx1UXWuN7hagrj4PD7+r/MXLJhycrj5ZqetdNkyZ4Pv3Y5fr56zh9+0aca14/ur06chNvZSQoiJudR8lCuX0/zifZSQXs0OK1Y43Tu3bXPaul9/3flfmSv7/Xfngs0RI5yTx/fd5ySC5s0z/jttCcCkK1Wn+eJKyeHi47ffLl14Bc4PITz80gbf39rx/Y2qc+XyxYSwYoXTdJSQ4EwvW/byo4Q773Suar1e8fHw6qtOR4ISJWDsWKcLrfHd6dMQGekcFezZ4xw9DRzoXGGcUd91SwDGLyQmOnuw+/c75yDuuivztuP7izNnYM2ay48SLp4zyZnTSbAXE0Lt2k7TnC82b3b2+tesgZ49nXNHdlX09btwwTkH9+67sH698394+mlnmIz0/lwtARgTIFSd6xW8jxLWrLnURFeq1OVHCdWqXb4nmpTkbOxfeME51zNypNOObdKGqtPs+e67zjmvPHmcJPCPf6Tf9ROWAIwJYOfOOU1F3kcJe/c607Jnd47Eatd2jhY++8wZPfehh5znRYu6GnqWtmaNM0je1KlOc2iXLvDss1C5ctquxxKAMeYy+/ZdOkKIiYHVq51EkTu3c6Vrr17+d01IVrVrlzO676hRTpNe8+bOCeN69dLmf2AJwBhzVfHxsHGj01vrb39zO5rAdOSI0832ww+dc2U1azqJoE0bCA6+/uVeKQEEcAc7Y4y30FCnX79t/N1TqJAzXMvu3c5IqseOOeMM3X67k5zTmk8JQEQiRGSriGwXkUEpTO8pIodEZJ3n8YinvJqIxIjIZhHZICIdveYZKyK/es2TyQZYNcaY9JEzJzz+uDPO0PTpcNttThfftJbqdYQiEgyMAJoCccAqEZmjqluSVZ2iqk8lKzsDdFfVbSJyM7BaRKJV9bhn+kBVnX6D78EYY7Kk4GBo29Z5pAdfjgBqAdtVdaeqxgOTgVa+LFxVf1HVbZ7n+4GDQNj1BmuMMSbt+JIAigN7vV7HecqSa+tp5pkuIiWTTxSRWkAosMOr+HXPPMNFxK7/NMaYDORLAkipE1LyrkNzgdKqeiewEPj8sgWIFAPGAb1U1TO0FYOBCkBNoCDwfIorF+krIrEiEnvIewxdY4wxN8SXBBAHeO/RlwD2e1dQ1SOqet7z8jPgzzECRSQv8BXwoqqu8JrngDrOA2Nwmpr+QlVHqmq4qoaHhVnrkTHGpBVfEsAqoJyIlBGRUKATMMe7gmcP/6KWwE+e8lBgFhClqtNSmkdEBGgNbLreN2GMMebapdoLSFUTROQpIBoIBkar6mYRGQLEquocoL+ItAQSgKNAT8/sHYB6QCERuVjWU1XXARNEJAyniWkd8HjavS1jjDGpsSuBjTEmi7MrgY0xxlwmUx0BiMghYPd1zl4YOJyG4WR29nlcYp/F5ezzuFxW+DxuUdW/9KLJVAngRohIbEqHQIHKPo9L7LO4nH0el8vKn4c1ARljTICyBGCMMQEqkBLASLcD8DP2eVxin8Xl7PO4XJb9PALmHIAxxpjLBdIRgDHGGC8BkQBSu6FNoBCRkiLynYj85LlJzz/cjskfiEiwiKwVkS/djsVtIpLfM6Lvz57vSR23Y3KLiAzw/E42icgkEcnhdkxpLcsnAK8b2jwAVAQ6i0hFd6NyTQLwT1W9A6gNPBnAn4W3f+AZv8rwPjBfVSsAVQnQz0VEigP9gXBVrYwzDE4nd6NKe1k+AXADN7TJajwjsK7xPD+F8+NO6d4OAUNESgAtgFFux+I2z8i99YBIAFWN97p7XyAKAXKKSAhwE8lGQc4KAiEB+HpDm4AiIqWB6sBKdyNx3XvAc0BSahUDQFngEDDG0yQ2SkRyuR2UG1R1H/BfYA9wADihqt+4G1XaC4QE4MsNbQKKiOQGZgBPq+pJt+Nxi4g8CBxU1dVux+InQoC7gI9VtTrwBxCQ58xEpABOS0EZ4GYgl4h0czeqtBcICSDVG9oEEhHJhrPxn6CqM92Ox2V1gZYisgunabCRiIx3NyRXxQFxqnrxqHA6TkIIRE2AX1X1kKpeAGYC97gcU5oLhASQ6g1tAoXn5juRwE+qOszteNymqoNVtYSqlsb5XixS1Sy3l+crVf0N2Csit3uKGgNbXAzJTXuA2iJyk+d305gseEI81RvCZHZXuqGNy2G5pS7wMLBRRNZ5yl5Q1XkuxmT8y//h3KwpFNgJ9HI5Hleo6koRmQ6swek9t5YseEWwXQlsjDEBKhCagIwxxqTAEoAxxgQoSwDGGBOgLAEYY0yAsgRgjDEByhKAMcYEKEsAxhgToCwBGGNMgPp/swHfVeyZah0AAAAASUVORK5CYII=\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.plot(trainLoss,color='g')\n",
    "plt.plot(validLoss,color='b')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test loss: 0.474\n",
      "Test accuracy: 0.392\n"
     ]
    }
   ],
   "source": [
    "#Loading saved model\n",
    "net.load_state_dict(torch.load('LSTM_Model.pt'))\n",
    "\n",
    "if(train_on_gpu):\n",
    "  net.to('cuda')\n",
    "# Get test data loss and accuracy\n",
    "\n",
    "test_losses = [] # track loss\n",
    "num_correct = 0\n",
    "\n",
    "# init hidden state\n",
    "h = net.init_hidden(batch_size)\n",
    "\n",
    "net.eval()\n",
    "# iterate over test data\n",
    "for inputs, labels in test_loader:\n",
    "\n",
    "    # Creating new variables for the hidden state, otherwise\n",
    "    # we'd backprop through the entire training history\n",
    "    h = tuple([each.data for each in h])\n",
    "\n",
    "    if(train_on_gpu):\n",
    "        inputs, labels = inputs.cuda().long(), labels.cuda().long()\n",
    "    \n",
    "    # get predicted outputs\n",
    "    output, h = net(inputs, h)\n",
    "    \n",
    "    # calculate loss\n",
    "    test_loss = criterion(output.squeeze(), labels.float())\n",
    "    test_losses.append(test_loss.item())\n",
    "    \n",
    "    # convert output probabilities to predicted class (0 or 1)\n",
    "    pred = torch.round(output.squeeze())  # rounds to the nearest integer\n",
    "    \n",
    "    # compare predictions to true label\n",
    "    correct_tensor = pred.eq(labels.float().view_as(pred))\n",
    "    correct = np.squeeze(correct_tensor.numpy()) if not train_on_gpu else np.squeeze(correct_tensor.cpu().numpy())\n",
    "    num_correct += np.sum(correct)\n",
    "\n",
    "\n",
    "# -- stats! -- ##\n",
    "# avg test loss\n",
    "print(\"Test loss: {:.3f}\".format(np.mean(test_losses)))\n",
    "\n",
    "# accuracy over all test data\n",
    "test_acc = num_correct/len(test_loader.dataset)\n",
    "print(\"Test accuracy: {:.3f}\".format(test_acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test loss: 0.474\n",
      "Test accuracy: 0.390\n"
     ]
    }
   ],
   "source": [
    "\n",
    "#Loading saved model\n",
    "net.load_state_dict(torch.load('LSTM_Model.pt'))\n",
    "\n",
    "if(train_on_gpu):\n",
    "  net.to('cuda')\n",
    "# Get test data loss and accuracy\n",
    "\n",
    "test_losses = [] # track loss\n",
    "num_correct = 0\n",
    "\n",
    "# init hidden state\n",
    "h = net.init_hidden(batch_size)\n",
    "\n",
    "net.eval()\n",
    "# iterate over test data\n",
    "for inputs, labels in test_loader:\n",
    "\n",
    "    # Creating new variables for the hidden state, otherwise\n",
    "    # we'd backprop through the entire training history\n",
    "    h = tuple([each.data for each in h])\n",
    "\n",
    "    if(train_on_gpu):\n",
    "        inputs, labels = inputs.cuda().long(), labels.cuda().long()\n",
    "    \n",
    "    # get predicted outputs\n",
    "    output, h = net(inputs, h)\n",
    "    \n",
    "    # calculate loss\n",
    "    test_loss = criterion(output.squeeze(), labels.float())\n",
    "    test_losses.append(test_loss.item())\n",
    "    \n",
    "    # convert output probabilities to predicted class (0 or 1)\n",
    "    pred = torch.round(output.squeeze())  # rounds to the nearest integer\n",
    "    \n",
    "    # compare predictions to true label\n",
    "    correct_tensor = pred.eq(labels.float().view_as(pred))\n",
    "    correct = np.squeeze(correct_tensor.numpy()) if not train_on_gpu else np.squeeze(correct_tensor.cpu().numpy())\n",
    "    num_correct += np.sum(correct)\n",
    "\n",
    "\n",
    "# -- stats! -- ##\n",
    "# avg test loss\n",
    "print(\"Test loss: {:.3f}\".format(np.mean(test_losses)))\n",
    "\n",
    "# accuracy over all test data\n",
    "test_acc = num_correct/len(test_loader.dataset)\n",
    "print(\"Test accuracy: {:.3f}\".format(test_acc))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
