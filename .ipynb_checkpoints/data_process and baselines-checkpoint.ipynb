{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## pre-pocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "import seaborn as sb\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "import matplotlib.pyplot as plt\n",
    "from numpy import random\n",
    "import gensim\n",
    "import nltk\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.ensemble import AdaBoostRegressor\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from gensim.models import Doc2Vec\n",
    "from gensim.models.doc2vec import TaggedDocument\n",
    "from gensim.models import Word2Vec\n",
    "from sklearn.svm import SVC\n",
    "import torch\n",
    "import torch.utils.data as Data\n",
    "import torchvision"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = \"dataset/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'1': 'Film & Animation', '2': 'Autos & Vehicles', '10': 'Music', '15': 'Pets & Animals', '17': 'Sports', '18': 'Short Movies', '19': 'Travel & Events', '20': 'Gaming', '21': 'Videoblogging', '22': 'People & Blogs', '23': 'Comedy', '24': 'Entertainment', '25': 'News & Politics', '26': 'Howto & Style', '27': 'Education', '28': 'Science & Technology', '29': 'Nonprofits & Activism', '30': 'Movies', '31': 'Anime/Animation', '32': 'Action/Adventure', '33': 'Classics', '34': 'Comedy', '35': 'Documentary', '36': 'Drama', '37': 'Family', '38': 'Foreign', '39': 'Horror', '40': 'Sci-Fi/Fantasy', '41': 'Thriller', '42': 'Shorts', '43': 'Shows', '44': 'Trailers'}\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv(path+\"USvideos.csv\")\n",
    "df = df.assign(country=\"US\")\n",
    "df['trending_date'] = pd.to_datetime(df['trending_date'], format='%y.%d.%m')  \n",
    "df.trending_date = df.trending_date.dt.date   \n",
    "df['publish_time'] = pd.to_datetime(df['publish_time'], format='%Y-%m-%dT%H:%M:%S.%fZ')\n",
    "df=df.assign(publish_date=df['publish_time'].dt.date)\n",
    "df['publish_time'] = df['publish_time'].dt.time\n",
    "\n",
    "###导入category名称###\n",
    "df=df.assign(cat_name='a')\n",
    "id_to_category = {}\n",
    "file=path+'US_category_id.json'\n",
    "with open(file, 'r') as f:\n",
    "    data=json.load(f)\n",
    "    for category in data['items']:\n",
    "        id_to_category[category['id']] = category['snippet']['title']\n",
    "print(id_to_category)\n",
    "###实际上每个国家的category id-name 字典是一样的\n",
    "df['category_id'] = df['category_id'].astype(str)\n",
    "df.insert(4, 'category', df['category_id'].map(id_to_category))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clean Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x20edb49c1c8>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "df_summary = df.describe(include=\"all\")\n",
    "\n",
    "# Use heatmap to check missing data\n",
    "sb.heatmap(df_summary.isnull(), yticklabels=False, cbar=False, cmap='viridis')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "video_id 7\n",
      "trending_date 7\n",
      "title 7\n",
      "channel_title 7\n",
      "category 7\n",
      "category_id 7\n",
      "publish_time 7\n",
      "tags 7\n",
      "views 3\n",
      "likes 3\n",
      "dislikes 3\n",
      "comment_count 3\n",
      "thumbnail_link 7\n",
      "comments_disabled 7\n",
      "ratings_disabled 7\n",
      "video_error_or_removed 7\n",
      "description 7\n",
      "country 7\n",
      "publish_date 7\n",
      "cat_name 7\n"
     ]
    }
   ],
   "source": [
    "# See counts of missing value\n",
    "for c in df_summary.columns:\n",
    "    print(c,np.sum(df_summary[c].isnull()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\n",
    "df.sample(5)\n",
    "# Replace missing data\n",
    "df_summary['description'].fillna(\"\", inplace=True)\n",
    "df_summary['title'].fillna(\"\", inplace=True)\n",
    "df_summary['category_id'].fillna(0, inplace=True)\n",
    "\n",
    "\n",
    "# for k,v in df_summary.items():\n",
    "#     print(type(df_summary[k]))\n",
    "#     print(\"hhh\")\n",
    "#     df_summary[k].fillna(int(df_summary[k].mean()), inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x20edbddd348>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Drop a column most value are missing\n",
    "# df_summary.drop(['thumbnail_link'], axis=1, inplace=True)\n",
    "sb.heatmap(df_summary.isnull(), yticklabels=False, cbar=False, cmap='viridis')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# normalize\n",
    "# Import the libraries\n",
    "\n",
    "# plt.plot(df['views'],df['view'])\n",
    "# matplotlib histogram\n",
    "plt.hist(df['views'], color = 'blue', edgecolor = 'black',\n",
    "         bins = int(30))\n",
    "\n",
    "# # seaborn histogram\n",
    "# sb.distplot(df['view'], hist=True, kde=False, \n",
    "#              bins=int(180/5), color = 'blue',\n",
    "#              hist_kws={'edgecolor':'black'})\n",
    "# Add labels\n",
    "plt.title('Histogram')\n",
    "plt.xlabel('views')\n",
    "plt.ylabel('number')\n",
    "\n",
    "# np.sum(df['views']>2.35e6)\n",
    "y_train = np.zeros_like(df['views'])\n",
    "y_train[df['views']>7e5] = 1\n",
    "y_train[df['views']>2.35e6] = 2\n",
    "# y_train.shape\n",
    "# np.median(df['views'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0        SHANTELL'S CHANNEL - https://www.youtube.com/s...\n",
       "1        One year after the presidential election, John...\n",
       "2        WATCH MY PREVIOUS VIDEO ▶ \\n\\nSUBSCRIBE ► http...\n",
       "3        Today we find out if Link is a Nickelback amat...\n",
       "4        I know it's been a while since we did this sho...\n",
       "                               ...                        \n",
       "40944       The Cat Who Caught the Laser - Aaron's Animals\n",
       "40945                                                  NaN\n",
       "40946    I had so much fun transforming Safiyas hair in...\n",
       "40947    How Black Panther Should Have EndedWatch More ...\n",
       "40948    Call of Duty: Black Ops 4 Multiplayer raises t...\n",
       "Name: description, Length: 40949, dtype: object"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['description']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4095"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data, test_data = train_test_split(df, test_size=0.1, random_state=42)\n",
    "train_X, test_X = train_test_split(df, test_size=0.1, random_state=42)\n",
    "train_y, test_y = train_test_split(y_train, test_size=0.1, random_state=42)\n",
    "len(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(40949,)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['views'].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Big of Words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_text(text):\n",
    "    tokens = []\n",
    "    for sent in nltk.sent_tokenize(text):\n",
    "        for word in nltk.word_tokenize(sent):\n",
    "            if len(word) < 2:\n",
    "                continue\n",
    "            tokens.append(word.lower())\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 55.7 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# training\n",
    "count_vectorizer = CountVectorizer(\n",
    "    analyzer=\"word\", tokenizer=nltk.word_tokenize,\n",
    "    preprocessor=None, stop_words='english', max_features=3000) \n",
    "train_data_features = count_vectorizer.fit_transform(train_data['description'].values.astype('U')) #3000*train_feature\n",
    "test_data_features = count_vectorizer.transform(test_data['description'].values.astype('U')) #3000*train_feature"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linear Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "reg = LinearRegression().fit(train_data_features, train_data['views'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.43283299166900924"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "reg.score(test_data_features, test_data['views'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## AdaBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.1690934171548062"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "regr = AdaBoostRegressor(n_estimators=500, random_state=0,learning_rate=1e-7,loss='square')\n",
    "regr.fit(train_data_features, train_data['views'])\n",
    "regr.score(test_data_features, test_data['views'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gradient Boost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6271319526630299"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "regr2 = GradientBoostingRegressor(n_estimators=500, random_state=0,learning_rate=0.1,loss='ls')\n",
    "regr2.fit(train_data_features, train_data['views'])\n",
    "regr2.score(test_data_features, test_data['views'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\gx\\Anaconda3\\lib\\site-packages\\sklearn\\svm\\base.py:193: FutureWarning: The default value of gamma will change from 'auto' to 'scale' in version 0.22 to account better for unscaled features. Set gamma explicitly to 'auto' or 'scale' to avoid this warning.\n",
      "  \"avoid this warning.\", FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LibSVM]"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.6332112332112332"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "len(test_X)\n",
    "\n",
    "clf = SVC(verbose=True)\n",
    "clf.fit(train_data_features, train_y) \n",
    "clf.score(test_data_features, test_y)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classification GB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7787545787545788"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf = GradientBoostingClassifier(n_estimators=500, random_state=0,learning_rate=0.1,loss='deviance')\n",
    "clf.fit(train_data_features, train_y) \n",
    "clf.score(test_data_features, test_y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Word Vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 35.7 s\n"
     ]
    }
   ],
   "source": [
    "%%time \n",
    "import gensim.downloader as api\n",
    "\n",
    "info = api.info()  # show info about available models/datasets\n",
    "model = api.load(\"glove-twitter-25\")  # download the model and return as object ready for use\n",
    "# model.most_similar(\"cat\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "def word_averaging(wv, words):\n",
    "    all_words, mean = set(), []\n",
    "    \n",
    "    for word in words:\n",
    "        if isinstance(word, np.ndarray):\n",
    "            mean.append(word)\n",
    "        elif word in wv.vocab:\n",
    "#             print(wv.vocab[word].index)\n",
    "            mean.append(wv.syn0norm[wv.vocab[word].index])\n",
    "            all_words.add(wv.vocab[word].index)\n",
    "\n",
    "    if not mean:\n",
    "#         logging.warning(\"cannot compute similarity with no input %s\", words)\n",
    "        # FIXME: remove these examples in pre-processing\n",
    "        return np.zeros(25,)\n",
    "\n",
    "    mean = gensim.matutils.unitvec(np.array(mean).mean(axis=0)).astype(np.float64)\n",
    "    return mean\n",
    "\n",
    "def  word_averaging_list(wv, text_list):\n",
    "    return np.vstack([word_averaging(wv, review) for review in text_list ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\gx\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:4: DeprecationWarning: Call to deprecated `syn0norm` (Attribute will be removed in 4.0.0, use self.vectors_norm instead).\n",
      "  after removing the cwd from sys.path.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(1193514, 25)"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from itertools import islice\n",
    "list(islice(model.vocab, 13000, 13020))\n",
    "model.most_similar(\"cat\")\n",
    "model.syn0norm.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def w2v_tokenize_text(text):\n",
    "    tokens = []\n",
    "    if not text:\n",
    "        print('The text to be tokenized is a None type. Defaulting to blank string.')\n",
    "        text = ''\n",
    "    for sent in nltk.sent_tokenize(text):\n",
    "        for word in nltk.word_tokenize(sent):\n",
    "            if len(word) < 2:\n",
    "                continue\n",
    "            tokens.append(word)\n",
    "    return tokens\n",
    "\n",
    "# def custom_tokenize(text):\n",
    "#     if not text:\n",
    "#         print('The text to be tokenized is a None type. Defaulting to blank string.')\n",
    "#         text = ''\n",
    "#     return word_tokenize(text)\n",
    "\n",
    "def tokenize_text(text):\n",
    "    tokens = []\n",
    "    for sent in nltk.sent_tokenize(text):\n",
    "        for word in nltk.word_tokenize(sent):\n",
    "            if len(word) < 2:\n",
    "                continue\n",
    "            tokens.append(word.lower())\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The text to be tokenized is a None type. Defaulting to blank string.\n",
      "The text to be tokenized is a None type. Defaulting to blank string.\n",
      "The text to be tokenized is a None type. Defaulting to blank string.\n",
      "The text to be tokenized is a None type. Defaulting to blank string.\n",
      "The text to be tokenized is a None type. Defaulting to blank string.\n",
      "The text to be tokenized is a None type. Defaulting to blank string.\n",
      "The text to be tokenized is a None type. Defaulting to blank string.\n",
      "The text to be tokenized is a None type. Defaulting to blank string.\n",
      "The text to be tokenized is a None type. Defaulting to blank string.\n",
      "The text to be tokenized is a None type. Defaulting to blank string.\n",
      "The text to be tokenized is a None type. Defaulting to blank string.\n",
      "The text to be tokenized is a None type. Defaulting to blank string.\n",
      "The text to be tokenized is a None type. Defaulting to blank string.\n",
      "The text to be tokenized is a None type. Defaulting to blank string.\n",
      "The text to be tokenized is a None type. Defaulting to blank string.\n",
      "The text to be tokenized is a None type. Defaulting to blank string.\n",
      "The text to be tokenized is a None type. Defaulting to blank string.\n",
      "The text to be tokenized is a None type. Defaulting to blank string.\n",
      "The text to be tokenized is a None type. Defaulting to blank string.\n",
      "The text to be tokenized is a None type. Defaulting to blank string.\n",
      "The text to be tokenized is a None type. Defaulting to blank string.\n",
      "The text to be tokenized is a None type. Defaulting to blank string.\n",
      "The text to be tokenized is a None type. Defaulting to blank string.\n",
      "The text to be tokenized is a None type. Defaulting to blank string.\n",
      "The text to be tokenized is a None type. Defaulting to blank string.\n",
      "The text to be tokenized is a None type. Defaulting to blank string.\n",
      "The text to be tokenized is a None type. Defaulting to blank string.\n",
      "The text to be tokenized is a None type. Defaulting to blank string.\n",
      "The text to be tokenized is a None type. Defaulting to blank string.\n",
      "The text to be tokenized is a None type. Defaulting to blank string.\n",
      "The text to be tokenized is a None type. Defaulting to blank string.\n",
      "The text to be tokenized is a None type. Defaulting to blank string.\n",
      "The text to be tokenized is a None type. Defaulting to blank string.\n",
      "The text to be tokenized is a None type. Defaulting to blank string.\n",
      "The text to be tokenized is a None type. Defaulting to blank string.\n",
      "The text to be tokenized is a None type. Defaulting to blank string.\n",
      "The text to be tokenized is a None type. Defaulting to blank string.\n",
      "The text to be tokenized is a None type. Defaulting to blank string.\n",
      "The text to be tokenized is a None type. Defaulting to blank string.\n",
      "The text to be tokenized is a None type. Defaulting to blank string.\n",
      "The text to be tokenized is a None type. Defaulting to blank string.\n",
      "The text to be tokenized is a None type. Defaulting to blank string.\n",
      "The text to be tokenized is a None type. Defaulting to blank string.\n",
      "The text to be tokenized is a None type. Defaulting to blank string.\n",
      "The text to be tokenized is a None type. Defaulting to blank string.\n",
      "The text to be tokenized is a None type. Defaulting to blank string.\n",
      "The text to be tokenized is a None type. Defaulting to blank string.\n",
      "The text to be tokenized is a None type. Defaulting to blank string.\n",
      "The text to be tokenized is a None type. Defaulting to blank string.\n",
      "The text to be tokenized is a None type. Defaulting to blank string.\n",
      "The text to be tokenized is a None type. Defaulting to blank string.\n",
      "The text to be tokenized is a None type. Defaulting to blank string.\n",
      "The text to be tokenized is a None type. Defaulting to blank string.\n",
      "The text to be tokenized is a None type. Defaulting to blank string.\n",
      "The text to be tokenized is a None type. Defaulting to blank string.\n",
      "The text to be tokenized is a None type. Defaulting to blank string.\n",
      "The text to be tokenized is a None type. Defaulting to blank string.\n",
      "The text to be tokenized is a None type. Defaulting to blank string.\n",
      "The text to be tokenized is a None type. Defaulting to blank string.\n",
      "The text to be tokenized is a None type. Defaulting to blank string.\n",
      "The text to be tokenized is a None type. Defaulting to blank string.\n",
      "The text to be tokenized is a None type. Defaulting to blank string.\n",
      "The text to be tokenized is a None type. Defaulting to blank string.\n",
      "The text to be tokenized is a None type. Defaulting to blank string.\n",
      "The text to be tokenized is a None type. Defaulting to blank string.\n",
      "The text to be tokenized is a None type. Defaulting to blank string.\n",
      "The text to be tokenized is a None type. Defaulting to blank string.\n",
      "The text to be tokenized is a None type. Defaulting to blank string.\n",
      "The text to be tokenized is a None type. Defaulting to blank string.\n",
      "The text to be tokenized is a None type. Defaulting to blank string.\n",
      "The text to be tokenized is a None type. Defaulting to blank string.\n",
      "The text to be tokenized is a None type. Defaulting to blank string.\n",
      "The text to be tokenized is a None type. Defaulting to blank string.\n",
      "The text to be tokenized is a None type. Defaulting to blank string.\n",
      "The text to be tokenized is a None type. Defaulting to blank string.\n",
      "The text to be tokenized is a None type. Defaulting to blank string.\n",
      "The text to be tokenized is a None type. Defaulting to blank string.\n",
      "The text to be tokenized is a None type. Defaulting to blank string.\n",
      "The text to be tokenized is a None type. Defaulting to blank string.\n",
      "The text to be tokenized is a None type. Defaulting to blank string.\n",
      "The text to be tokenized is a None type. Defaulting to blank string.\n",
      "The text to be tokenized is a None type. Defaulting to blank string.\n",
      "The text to be tokenized is a None type. Defaulting to blank string.\n",
      "The text to be tokenized is a None type. Defaulting to blank string.\n",
      "The text to be tokenized is a None type. Defaulting to blank string.\n",
      "The text to be tokenized is a None type. Defaulting to blank string.\n",
      "The text to be tokenized is a None type. Defaulting to blank string.\n",
      "The text to be tokenized is a None type. Defaulting to blank string.\n",
      "The text to be tokenized is a None type. Defaulting to blank string.\n",
      "The text to be tokenized is a None type. Defaulting to blank string.\n",
      "The text to be tokenized is a None type. Defaulting to blank string.\n",
      "The text to be tokenized is a None type. Defaulting to blank string.\n",
      "The text to be tokenized is a None type. Defaulting to blank string.\n",
      "The text to be tokenized is a None type. Defaulting to blank string.\n",
      "The text to be tokenized is a None type. Defaulting to blank string.\n",
      "The text to be tokenized is a None type. Defaulting to blank string.\n",
      "The text to be tokenized is a None type. Defaulting to blank string.\n",
      "The text to be tokenized is a None type. Defaulting to blank string.\n",
      "The text to be tokenized is a None type. Defaulting to blank string.\n",
      "The text to be tokenized is a None type. Defaulting to blank string.\n",
      "The text to be tokenized is a None type. Defaulting to blank string.\n",
      "The text to be tokenized is a None type. Defaulting to blank string.\n",
      "The text to be tokenized is a None type. Defaulting to blank string.\n",
      "The text to be tokenized is a None type. Defaulting to blank string.\n",
      "The text to be tokenized is a None type. Defaulting to blank string.\n",
      "The text to be tokenized is a None type. Defaulting to blank string.\n",
      "The text to be tokenized is a None type. Defaulting to blank string.\n",
      "The text to be tokenized is a None type. Defaulting to blank string.\n",
      "The text to be tokenized is a None type. Defaulting to blank string.\n",
      "The text to be tokenized is a None type. Defaulting to blank string.\n",
      "The text to be tokenized is a None type. Defaulting to blank string.\n",
      "The text to be tokenized is a None type. Defaulting to blank string.\n",
      "The text to be tokenized is a None type. Defaulting to blank string.\n",
      "The text to be tokenized is a None type. Defaulting to blank string.\n",
      "The text to be tokenized is a None type. Defaulting to blank string.\n",
      "The text to be tokenized is a None type. Defaulting to blank string.\n",
      "The text to be tokenized is a None type. Defaulting to blank string.\n",
      "The text to be tokenized is a None type. Defaulting to blank string.\n",
      "The text to be tokenized is a None type. Defaulting to blank string.\n",
      "The text to be tokenized is a None type. Defaulting to blank string.\n",
      "The text to be tokenized is a None type. Defaulting to blank string.\n",
      "The text to be tokenized is a None type. Defaulting to blank string.\n",
      "The text to be tokenized is a None type. Defaulting to blank string.\n",
      "The text to be tokenized is a None type. Defaulting to blank string.\n",
      "The text to be tokenized is a None type. Defaulting to blank string.\n",
      "The text to be tokenized is a None type. Defaulting to blank string.\n",
      "The text to be tokenized is a None type. Defaulting to blank string.\n",
      "The text to be tokenized is a None type. Defaulting to blank string.\n",
      "The text to be tokenized is a None type. Defaulting to blank string.\n",
      "The text to be tokenized is a None type. Defaulting to blank string.\n",
      "The text to be tokenized is a None type. Defaulting to blank string.\n",
      "The text to be tokenized is a None type. Defaulting to blank string.\n",
      "The text to be tokenized is a None type. Defaulting to blank string.\n",
      "The text to be tokenized is a None type. Defaulting to blank string.\n",
      "The text to be tokenized is a None type. Defaulting to blank string.\n",
      "The text to be tokenized is a None type. Defaulting to blank string.\n",
      "The text to be tokenized is a None type. Defaulting to blank string.\n",
      "The text to be tokenized is a None type. Defaulting to blank string.\n",
      "The text to be tokenized is a None type. Defaulting to blank string.\n",
      "The text to be tokenized is a None type. Defaulting to blank string.\n",
      "The text to be tokenized is a None type. Defaulting to blank string.\n",
      "The text to be tokenized is a None type. Defaulting to blank string.\n",
      "The text to be tokenized is a None type. Defaulting to blank string.\n",
      "The text to be tokenized is a None type. Defaulting to blank string.\n",
      "The text to be tokenized is a None type. Defaulting to blank string.\n",
      "The text to be tokenized is a None type. Defaulting to blank string.\n",
      "The text to be tokenized is a None type. Defaulting to blank string.\n",
      "The text to be tokenized is a None type. Defaulting to blank string.\n",
      "The text to be tokenized is a None type. Defaulting to blank string.\n",
      "The text to be tokenized is a None type. Defaulting to blank string.\n",
      "The text to be tokenized is a None type. Defaulting to blank string.\n",
      "The text to be tokenized is a None type. Defaulting to blank string.\n",
      "The text to be tokenized is a None type. Defaulting to blank string.\n",
      "The text to be tokenized is a None type. Defaulting to blank string.\n",
      "The text to be tokenized is a None type. Defaulting to blank string.\n",
      "The text to be tokenized is a None type. Defaulting to blank string.\n",
      "The text to be tokenized is a None type. Defaulting to blank string.\n",
      "The text to be tokenized is a None type. Defaulting to blank string.\n",
      "The text to be tokenized is a None type. Defaulting to blank string.\n",
      "The text to be tokenized is a None type. Defaulting to blank string.\n",
      "The text to be tokenized is a None type. Defaulting to blank string.\n",
      "The text to be tokenized is a None type. Defaulting to blank string.\n",
      "The text to be tokenized is a None type. Defaulting to blank string.\n",
      "The text to be tokenized is a None type. Defaulting to blank string.\n",
      "The text to be tokenized is a None type. Defaulting to blank string.\n",
      "The text to be tokenized is a None type. Defaulting to blank string.\n",
      "The text to be tokenized is a None type. Defaulting to blank string.\n",
      "The text to be tokenized is a None type. Defaulting to blank string.\n",
      "The text to be tokenized is a None type. Defaulting to blank string.\n",
      "The text to be tokenized is a None type. Defaulting to blank string.\n",
      "The text to be tokenized is a None type. Defaulting to blank string.\n",
      "The text to be tokenized is a None type. Defaulting to blank string.\n",
      "The text to be tokenized is a None type. Defaulting to blank string.\n",
      "The text to be tokenized is a None type. Defaulting to blank string.\n",
      "The text to be tokenized is a None type. Defaulting to blank string.\n",
      "The text to be tokenized is a None type. Defaulting to blank string.\n",
      "The text to be tokenized is a None type. Defaulting to blank string.\n",
      "The text to be tokenized is a None type. Defaulting to blank string.\n",
      "The text to be tokenized is a None type. Defaulting to blank string.\n",
      "The text to be tokenized is a None type. Defaulting to blank string.\n",
      "The text to be tokenized is a None type. Defaulting to blank string.\n",
      "The text to be tokenized is a None type. Defaulting to blank string.\n",
      "The text to be tokenized is a None type. Defaulting to blank string.\n",
      "The text to be tokenized is a None type. Defaulting to blank string.\n",
      "The text to be tokenized is a None type. Defaulting to blank string.\n",
      "The text to be tokenized is a None type. Defaulting to blank string.\n",
      "The text to be tokenized is a None type. Defaulting to blank string.\n",
      "The text to be tokenized is a None type. Defaulting to blank string.\n",
      "The text to be tokenized is a None type. Defaulting to blank string.\n",
      "The text to be tokenized is a None type. Defaulting to blank string.\n",
      "The text to be tokenized is a None type. Defaulting to blank string.\n",
      "The text to be tokenized is a None type. Defaulting to blank string.\n",
      "The text to be tokenized is a None type. Defaulting to blank string.\n",
      "The text to be tokenized is a None type. Defaulting to blank string.\n",
      "The text to be tokenized is a None type. Defaulting to blank string.\n",
      "The text to be tokenized is a None type. Defaulting to blank string.\n",
      "The text to be tokenized is a None type. Defaulting to blank string.\n",
      "The text to be tokenized is a None type. Defaulting to blank string.\n",
      "The text to be tokenized is a None type. Defaulting to blank string.\n",
      "The text to be tokenized is a None type. Defaulting to blank string.\n",
      "The text to be tokenized is a None type. Defaulting to blank string.\n",
      "The text to be tokenized is a None type. Defaulting to blank string.\n",
      "The text to be tokenized is a None type. Defaulting to blank string.\n",
      "The text to be tokenized is a None type. Defaulting to blank string.\n",
      "The text to be tokenized is a None type. Defaulting to blank string.\n",
      "The text to be tokenized is a None type. Defaulting to blank string.\n",
      "The text to be tokenized is a None type. Defaulting to blank string.\n",
      "The text to be tokenized is a None type. Defaulting to blank string.\n",
      "The text to be tokenized is a None type. Defaulting to blank string.\n",
      "The text to be tokenized is a None type. Defaulting to blank string.\n",
      "The text to be tokenized is a None type. Defaulting to blank string.\n",
      "The text to be tokenized is a None type. Defaulting to blank string.\n",
      "The text to be tokenized is a None type. Defaulting to blank string.\n",
      "The text to be tokenized is a None type. Defaulting to blank string.\n",
      "The text to be tokenized is a None type. Defaulting to blank string.\n",
      "The text to be tokenized is a None type. Defaulting to blank string.\n",
      "The text to be tokenized is a None type. Defaulting to blank string.\n",
      "The text to be tokenized is a None type. Defaulting to blank string.\n",
      "The text to be tokenized is a None type. Defaulting to blank string.\n",
      "The text to be tokenized is a None type. Defaulting to blank string.\n",
      "The text to be tokenized is a None type. Defaulting to blank string.\n",
      "The text to be tokenized is a None type. Defaulting to blank string.\n",
      "The text to be tokenized is a None type. Defaulting to blank string.\n",
      "The text to be tokenized is a None type. Defaulting to blank string.\n",
      "The text to be tokenized is a None type. Defaulting to blank string.\n",
      "The text to be tokenized is a None type. Defaulting to blank string.\n",
      "The text to be tokenized is a None type. Defaulting to blank string.\n",
      "The text to be tokenized is a None type. Defaulting to blank string.\n",
      "The text to be tokenized is a None type. Defaulting to blank string.\n",
      "The text to be tokenized is a None type. Defaulting to blank string.\n",
      "The text to be tokenized is a None type. Defaulting to blank string.\n",
      "The text to be tokenized is a None type. Defaulting to blank string.\n",
      "The text to be tokenized is a None type. Defaulting to blank string.\n",
      "The text to be tokenized is a None type. Defaulting to blank string.\n",
      "The text to be tokenized is a None type. Defaulting to blank string.\n",
      "The text to be tokenized is a None type. Defaulting to blank string.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The text to be tokenized is a None type. Defaulting to blank string.\n",
      "The text to be tokenized is a None type. Defaulting to blank string.\n",
      "The text to be tokenized is a None type. Defaulting to blank string.\n",
      "The text to be tokenized is a None type. Defaulting to blank string.\n",
      "The text to be tokenized is a None type. Defaulting to blank string.\n",
      "The text to be tokenized is a None type. Defaulting to blank string.\n",
      "The text to be tokenized is a None type. Defaulting to blank string.\n",
      "The text to be tokenized is a None type. Defaulting to blank string.\n",
      "The text to be tokenized is a None type. Defaulting to blank string.\n",
      "The text to be tokenized is a None type. Defaulting to blank string.\n",
      "The text to be tokenized is a None type. Defaulting to blank string.\n",
      "The text to be tokenized is a None type. Defaulting to blank string.\n",
      "The text to be tokenized is a None type. Defaulting to blank string.\n",
      "The text to be tokenized is a None type. Defaulting to blank string.\n",
      "The text to be tokenized is a None type. Defaulting to blank string.\n",
      "The text to be tokenized is a None type. Defaulting to blank string.\n",
      "The text to be tokenized is a None type. Defaulting to blank string.\n",
      "The text to be tokenized is a None type. Defaulting to blank string.\n",
      "The text to be tokenized is a None type. Defaulting to blank string.\n",
      "The text to be tokenized is a None type. Defaulting to blank string.\n",
      "The text to be tokenized is a None type. Defaulting to blank string.\n",
      "The text to be tokenized is a None type. Defaulting to blank string.\n",
      "The text to be tokenized is a None type. Defaulting to blank string.\n",
      "The text to be tokenized is a None type. Defaulting to blank string.\n",
      "The text to be tokenized is a None type. Defaulting to blank string.\n",
      "The text to be tokenized is a None type. Defaulting to blank string.\n",
      "The text to be tokenized is a None type. Defaulting to blank string.\n",
      "The text to be tokenized is a None type. Defaulting to blank string.\n",
      "The text to be tokenized is a None type. Defaulting to blank string.\n",
      "The text to be tokenized is a None type. Defaulting to blank string.\n",
      "The text to be tokenized is a None type. Defaulting to blank string.\n",
      "The text to be tokenized is a None type. Defaulting to blank string.\n",
      "The text to be tokenized is a None type. Defaulting to blank string.\n",
      "The text to be tokenized is a None type. Defaulting to blank string.\n",
      "The text to be tokenized is a None type. Defaulting to blank string.\n",
      "The text to be tokenized is a None type. Defaulting to blank string.\n",
      "The text to be tokenized is a None type. Defaulting to blank string.\n",
      "The text to be tokenized is a None type. Defaulting to blank string.\n",
      "The text to be tokenized is a None type. Defaulting to blank string.\n",
      "The text to be tokenized is a None type. Defaulting to blank string.\n",
      "The text to be tokenized is a None type. Defaulting to blank string.\n",
      "The text to be tokenized is a None type. Defaulting to blank string.\n",
      "The text to be tokenized is a None type. Defaulting to blank string.\n",
      "The text to be tokenized is a None type. Defaulting to blank string.\n",
      "The text to be tokenized is a None type. Defaulting to blank string.\n",
      "The text to be tokenized is a None type. Defaulting to blank string.\n",
      "The text to be tokenized is a None type. Defaulting to blank string.\n",
      "The text to be tokenized is a None type. Defaulting to blank string.\n",
      "The text to be tokenized is a None type. Defaulting to blank string.\n",
      "The text to be tokenized is a None type. Defaulting to blank string.\n",
      "The text to be tokenized is a None type. Defaulting to blank string.\n",
      "The text to be tokenized is a None type. Defaulting to blank string.\n",
      "The text to be tokenized is a None type. Defaulting to blank string.\n",
      "The text to be tokenized is a None type. Defaulting to blank string.\n",
      "The text to be tokenized is a None type. Defaulting to blank string.\n",
      "The text to be tokenized is a None type. Defaulting to blank string.\n",
      "The text to be tokenized is a None type. Defaulting to blank string.\n",
      "The text to be tokenized is a None type. Defaulting to blank string.\n",
      "The text to be tokenized is a None type. Defaulting to blank string.\n",
      "The text to be tokenized is a None type. Defaulting to blank string.\n",
      "The text to be tokenized is a None type. Defaulting to blank string.\n",
      "The text to be tokenized is a None type. Defaulting to blank string.\n",
      "The text to be tokenized is a None type. Defaulting to blank string.\n",
      "The text to be tokenized is a None type. Defaulting to blank string.\n",
      "The text to be tokenized is a None type. Defaulting to blank string.\n",
      "The text to be tokenized is a None type. Defaulting to blank string.\n",
      "The text to be tokenized is a None type. Defaulting to blank string.\n",
      "The text to be tokenized is a None type. Defaulting to blank string.\n",
      "The text to be tokenized is a None type. Defaulting to blank string.\n",
      "The text to be tokenized is a None type. Defaulting to blank string.\n",
      "The text to be tokenized is a None type. Defaulting to blank string.\n",
      "The text to be tokenized is a None type. Defaulting to blank string.\n",
      "The text to be tokenized is a None type. Defaulting to blank string.\n",
      "The text to be tokenized is a None type. Defaulting to blank string.\n",
      "The text to be tokenized is a None type. Defaulting to blank string.\n",
      "The text to be tokenized is a None type. Defaulting to blank string.\n",
      "The text to be tokenized is a None type. Defaulting to blank string.\n",
      "The text to be tokenized is a None type. Defaulting to blank string.\n",
      "The text to be tokenized is a None type. Defaulting to blank string.\n",
      "The text to be tokenized is a None type. Defaulting to blank string.\n",
      "The text to be tokenized is a None type. Defaulting to blank string.\n",
      "The text to be tokenized is a None type. Defaulting to blank string.\n",
      "The text to be tokenized is a None type. Defaulting to blank string.\n",
      "The text to be tokenized is a None type. Defaulting to blank string.\n",
      "The text to be tokenized is a None type. Defaulting to blank string.\n",
      "The text to be tokenized is a None type. Defaulting to blank string.\n",
      "The text to be tokenized is a None type. Defaulting to blank string.\n",
      "The text to be tokenized is a None type. Defaulting to blank string.\n",
      "The text to be tokenized is a None type. Defaulting to blank string.\n",
      "The text to be tokenized is a None type. Defaulting to blank string.\n",
      "The text to be tokenized is a None type. Defaulting to blank string.\n",
      "The text to be tokenized is a None type. Defaulting to blank string.\n",
      "The text to be tokenized is a None type. Defaulting to blank string.\n",
      "The text to be tokenized is a None type. Defaulting to blank string.\n",
      "The text to be tokenized is a None type. Defaulting to blank string.\n",
      "The text to be tokenized is a None type. Defaulting to blank string.\n",
      "The text to be tokenized is a None type. Defaulting to blank string.\n",
      "The text to be tokenized is a None type. Defaulting to blank string.\n",
      "The text to be tokenized is a None type. Defaulting to blank string.\n",
      "The text to be tokenized is a None type. Defaulting to blank string.\n",
      "The text to be tokenized is a None type. Defaulting to blank string.\n",
      "The text to be tokenized is a None type. Defaulting to blank string.\n",
      "The text to be tokenized is a None type. Defaulting to blank string.\n",
      "The text to be tokenized is a None type. Defaulting to blank string.\n",
      "The text to be tokenized is a None type. Defaulting to blank string.\n",
      "The text to be tokenized is a None type. Defaulting to blank string.\n",
      "The text to be tokenized is a None type. Defaulting to blank string.\n",
      "The text to be tokenized is a None type. Defaulting to blank string.\n",
      "The text to be tokenized is a None type. Defaulting to blank string.\n",
      "The text to be tokenized is a None type. Defaulting to blank string.\n",
      "The text to be tokenized is a None type. Defaulting to blank string.\n",
      "The text to be tokenized is a None type. Defaulting to blank string.\n",
      "The text to be tokenized is a None type. Defaulting to blank string.\n",
      "The text to be tokenized is a None type. Defaulting to blank string.\n",
      "The text to be tokenized is a None type. Defaulting to blank string.\n",
      "The text to be tokenized is a None type. Defaulting to blank string.\n",
      "The text to be tokenized is a None type. Defaulting to blank string.\n",
      "The text to be tokenized is a None type. Defaulting to blank string.\n",
      "The text to be tokenized is a None type. Defaulting to blank string.\n",
      "The text to be tokenized is a None type. Defaulting to blank string.\n",
      "The text to be tokenized is a None type. Defaulting to blank string.\n",
      "The text to be tokenized is a None type. Defaulting to blank string.\n",
      "The text to be tokenized is a None type. Defaulting to blank string.\n",
      "The text to be tokenized is a None type. Defaulting to blank string.\n",
      "The text to be tokenized is a None type. Defaulting to blank string.\n",
      "The text to be tokenized is a None type. Defaulting to blank string.\n",
      "The text to be tokenized is a None type. Defaulting to blank string.\n",
      "The text to be tokenized is a None type. Defaulting to blank string.\n",
      "The text to be tokenized is a None type. Defaulting to blank string.\n",
      "The text to be tokenized is a None type. Defaulting to blank string.\n",
      "The text to be tokenized is a None type. Defaulting to blank string.\n",
      "The text to be tokenized is a None type. Defaulting to blank string.\n",
      "The text to be tokenized is a None type. Defaulting to blank string.\n",
      "The text to be tokenized is a None type. Defaulting to blank string.\n",
      "The text to be tokenized is a None type. Defaulting to blank string.\n",
      "The text to be tokenized is a None type. Defaulting to blank string.\n",
      "The text to be tokenized is a None type. Defaulting to blank string.\n",
      "The text to be tokenized is a None type. Defaulting to blank string.\n",
      "The text to be tokenized is a None type. Defaulting to blank string.\n",
      "The text to be tokenized is a None type. Defaulting to blank string.\n",
      "The text to be tokenized is a None type. Defaulting to blank string.\n",
      "The text to be tokenized is a None type. Defaulting to blank string.\n",
      "The text to be tokenized is a None type. Defaulting to blank string.\n",
      "The text to be tokenized is a None type. Defaulting to blank string.\n",
      "The text to be tokenized is a None type. Defaulting to blank string.\n",
      "The text to be tokenized is a None type. Defaulting to blank string.\n",
      "The text to be tokenized is a None type. Defaulting to blank string.\n",
      "The text to be tokenized is a None type. Defaulting to blank string.\n",
      "The text to be tokenized is a None type. Defaulting to blank string.\n",
      "The text to be tokenized is a None type. Defaulting to blank string.\n",
      "The text to be tokenized is a None type. Defaulting to blank string.\n",
      "The text to be tokenized is a None type. Defaulting to blank string.\n",
      "The text to be tokenized is a None type. Defaulting to blank string.\n",
      "The text to be tokenized is a None type. Defaulting to blank string.\n",
      "The text to be tokenized is a None type. Defaulting to blank string.\n",
      "The text to be tokenized is a None type. Defaulting to blank string.\n",
      "The text to be tokenized is a None type. Defaulting to blank string.\n",
      "The text to be tokenized is a None type. Defaulting to blank string.\n",
      "The text to be tokenized is a None type. Defaulting to blank string.\n",
      "The text to be tokenized is a None type. Defaulting to blank string.\n",
      "The text to be tokenized is a None type. Defaulting to blank string.\n",
      "The text to be tokenized is a None type. Defaulting to blank string.\n",
      "The text to be tokenized is a None type. Defaulting to blank string.\n",
      "The text to be tokenized is a None type. Defaulting to blank string.\n",
      "The text to be tokenized is a None type. Defaulting to blank string.\n",
      "The text to be tokenized is a None type. Defaulting to blank string.\n",
      "The text to be tokenized is a None type. Defaulting to blank string.\n",
      "The text to be tokenized is a None type. Defaulting to blank string.\n",
      "The text to be tokenized is a None type. Defaulting to blank string.\n",
      "The text to be tokenized is a None type. Defaulting to blank string.\n",
      "The text to be tokenized is a None type. Defaulting to blank string.\n",
      "The text to be tokenized is a None type. Defaulting to blank string.\n",
      "The text to be tokenized is a None type. Defaulting to blank string.\n",
      "The text to be tokenized is a None type. Defaulting to blank string.\n",
      "The text to be tokenized is a None type. Defaulting to blank string.\n",
      "The text to be tokenized is a None type. Defaulting to blank string.\n",
      "The text to be tokenized is a None type. Defaulting to blank string.\n",
      "The text to be tokenized is a None type. Defaulting to blank string.\n",
      "The text to be tokenized is a None type. Defaulting to blank string.\n",
      "The text to be tokenized is a None type. Defaulting to blank string.\n",
      "The text to be tokenized is a None type. Defaulting to blank string.\n",
      "The text to be tokenized is a None type. Defaulting to blank string.\n",
      "The text to be tokenized is a None type. Defaulting to blank string.\n",
      "The text to be tokenized is a None type. Defaulting to blank string.\n",
      "The text to be tokenized is a None type. Defaulting to blank string.\n",
      "The text to be tokenized is a None type. Defaulting to blank string.\n",
      "The text to be tokenized is a None type. Defaulting to blank string.\n",
      "The text to be tokenized is a None type. Defaulting to blank string.\n",
      "The text to be tokenized is a None type. Defaulting to blank string.\n",
      "The text to be tokenized is a None type. Defaulting to blank string.\n",
      "The text to be tokenized is a None type. Defaulting to blank string.\n",
      "The text to be tokenized is a None type. Defaulting to blank string.\n",
      "The text to be tokenized is a None type. Defaulting to blank string.\n",
      "The text to be tokenized is a None type. Defaulting to blank string.\n",
      "The text to be tokenized is a None type. Defaulting to blank string.\n",
      "The text to be tokenized is a None type. Defaulting to blank string.\n",
      "The text to be tokenized is a None type. Defaulting to blank string.\n",
      "The text to be tokenized is a None type. Defaulting to blank string.\n",
      "The text to be tokenized is a None type. Defaulting to blank string.\n",
      "The text to be tokenized is a None type. Defaulting to blank string.\n",
      "The text to be tokenized is a None type. Defaulting to blank string.\n",
      "The text to be tokenized is a None type. Defaulting to blank string.\n",
      "The text to be tokenized is a None type. Defaulting to blank string.\n",
      "The text to be tokenized is a None type. Defaulting to blank string.\n",
      "The text to be tokenized is a None type. Defaulting to blank string.\n",
      "The text to be tokenized is a None type. Defaulting to blank string.\n",
      "The text to be tokenized is a None type. Defaulting to blank string.\n",
      "The text to be tokenized is a None type. Defaulting to blank string.\n",
      "The text to be tokenized is a None type. Defaulting to blank string.\n",
      "The text to be tokenized is a None type. Defaulting to blank string.\n",
      "The text to be tokenized is a None type. Defaulting to blank string.\n",
      "The text to be tokenized is a None type. Defaulting to blank string.\n",
      "The text to be tokenized is a None type. Defaulting to blank string.\n",
      "The text to be tokenized is a None type. Defaulting to blank string.\n",
      "The text to be tokenized is a None type. Defaulting to blank string.\n",
      "The text to be tokenized is a None type. Defaulting to blank string.\n",
      "The text to be tokenized is a None type. Defaulting to blank string.\n",
      "The text to be tokenized is a None type. Defaulting to blank string.\n",
      "The text to be tokenized is a None type. Defaulting to blank string.\n",
      "The text to be tokenized is a None type. Defaulting to blank string.\n",
      "The text to be tokenized is a None type. Defaulting to blank string.\n",
      "The text to be tokenized is a None type. Defaulting to blank string.\n",
      "The text to be tokenized is a None type. Defaulting to blank string.\n",
      "The text to be tokenized is a None type. Defaulting to blank string.\n",
      "The text to be tokenized is a None type. Defaulting to blank string.\n",
      "The text to be tokenized is a None type. Defaulting to blank string.\n",
      "The text to be tokenized is a None type. Defaulting to blank string.\n",
      "The text to be tokenized is a None type. Defaulting to blank string.\n",
      "The text to be tokenized is a None type. Defaulting to blank string.\n",
      "The text to be tokenized is a None type. Defaulting to blank string.\n",
      "The text to be tokenized is a None type. Defaulting to blank string.\n",
      "The text to be tokenized is a None type. Defaulting to blank string.\n",
      "The text to be tokenized is a None type. Defaulting to blank string.\n",
      "The text to be tokenized is a None type. Defaulting to blank string.\n",
      "The text to be tokenized is a None type. Defaulting to blank string.\n",
      "The text to be tokenized is a None type. Defaulting to blank string.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The text to be tokenized is a None type. Defaulting to blank string.\n",
      "The text to be tokenized is a None type. Defaulting to blank string.\n",
      "The text to be tokenized is a None type. Defaulting to blank string.\n",
      "The text to be tokenized is a None type. Defaulting to blank string.\n",
      "The text to be tokenized is a None type. Defaulting to blank string.\n",
      "The text to be tokenized is a None type. Defaulting to blank string.\n",
      "The text to be tokenized is a None type. Defaulting to blank string.\n",
      "The text to be tokenized is a None type. Defaulting to blank string.\n",
      "The text to be tokenized is a None type. Defaulting to blank string.\n",
      "The text to be tokenized is a None type. Defaulting to blank string.\n",
      "The text to be tokenized is a None type. Defaulting to blank string.\n",
      "The text to be tokenized is a None type. Defaulting to blank string.\n",
      "The text to be tokenized is a None type. Defaulting to blank string.\n",
      "The text to be tokenized is a None type. Defaulting to blank string.\n",
      "The text to be tokenized is a None type. Defaulting to blank string.\n",
      "The text to be tokenized is a None type. Defaulting to blank string.\n",
      "The text to be tokenized is a None type. Defaulting to blank string.\n",
      "The text to be tokenized is a None type. Defaulting to blank string.\n",
      "The text to be tokenized is a None type. Defaulting to blank string.\n",
      "The text to be tokenized is a None type. Defaulting to blank string.\n",
      "The text to be tokenized is a None type. Defaulting to blank string.\n",
      "The text to be tokenized is a None type. Defaulting to blank string.\n",
      "The text to be tokenized is a None type. Defaulting to blank string.\n",
      "The text to be tokenized is a None type. Defaulting to blank string.\n",
      "The text to be tokenized is a None type. Defaulting to blank string.\n",
      "The text to be tokenized is a None type. Defaulting to blank string.\n",
      "The text to be tokenized is a None type. Defaulting to blank string.\n",
      "The text to be tokenized is a None type. Defaulting to blank string.\n",
      "The text to be tokenized is a None type. Defaulting to blank string.\n",
      "The text to be tokenized is a None type. Defaulting to blank string.\n",
      "The text to be tokenized is a None type. Defaulting to blank string.\n",
      "The text to be tokenized is a None type. Defaulting to blank string.\n",
      "The text to be tokenized is a None type. Defaulting to blank string.\n",
      "The text to be tokenized is a None type. Defaulting to blank string.\n",
      "The text to be tokenized is a None type. Defaulting to blank string.\n",
      "The text to be tokenized is a None type. Defaulting to blank string.\n",
      "The text to be tokenized is a None type. Defaulting to blank string.\n",
      "The text to be tokenized is a None type. Defaulting to blank string.\n",
      "The text to be tokenized is a None type. Defaulting to blank string.\n",
      "The text to be tokenized is a None type. Defaulting to blank string.\n",
      "The text to be tokenized is a None type. Defaulting to blank string.\n",
      "The text to be tokenized is a None type. Defaulting to blank string.\n",
      "The text to be tokenized is a None type. Defaulting to blank string.\n",
      "The text to be tokenized is a None type. Defaulting to blank string.\n",
      "The text to be tokenized is a None type. Defaulting to blank string.\n",
      "The text to be tokenized is a None type. Defaulting to blank string.\n",
      "The text to be tokenized is a None type. Defaulting to blank string.\n",
      "The text to be tokenized is a None type. Defaulting to blank string.\n",
      "The text to be tokenized is a None type. Defaulting to blank string.\n",
      "The text to be tokenized is a None type. Defaulting to blank string.\n",
      "The text to be tokenized is a None type. Defaulting to blank string.\n",
      "The text to be tokenized is a None type. Defaulting to blank string.\n",
      "The text to be tokenized is a None type. Defaulting to blank string.\n",
      "The text to be tokenized is a None type. Defaulting to blank string.\n",
      "The text to be tokenized is a None type. Defaulting to blank string.\n",
      "The text to be tokenized is a None type. Defaulting to blank string.\n",
      "The text to be tokenized is a None type. Defaulting to blank string.\n",
      "The text to be tokenized is a None type. Defaulting to blank string.\n",
      "The text to be tokenized is a None type. Defaulting to blank string.\n",
      "The text to be tokenized is a None type. Defaulting to blank string.\n",
      "The text to be tokenized is a None type. Defaulting to blank string.\n",
      "The text to be tokenized is a None type. Defaulting to blank string.\n",
      "The text to be tokenized is a None type. Defaulting to blank string.\n",
      "The text to be tokenized is a None type. Defaulting to blank string.\n",
      "The text to be tokenized is a None type. Defaulting to blank string.\n",
      "The text to be tokenized is a None type. Defaulting to blank string.\n",
      "The text to be tokenized is a None type. Defaulting to blank string.\n",
      "The text to be tokenized is a None type. Defaulting to blank string.\n",
      "The text to be tokenized is a None type. Defaulting to blank string.\n",
      "The text to be tokenized is a None type. Defaulting to blank string.\n",
      "The text to be tokenized is a None type. Defaulting to blank string.\n",
      "The text to be tokenized is a None type. Defaulting to blank string.\n",
      "The text to be tokenized is a None type. Defaulting to blank string.\n",
      "The text to be tokenized is a None type. Defaulting to blank string.\n",
      "The text to be tokenized is a None type. Defaulting to blank string.\n",
      "The text to be tokenized is a None type. Defaulting to blank string.\n",
      "The text to be tokenized is a None type. Defaulting to blank string.\n",
      "The text to be tokenized is a None type. Defaulting to blank string.\n",
      "The text to be tokenized is a None type. Defaulting to blank string.\n",
      "The text to be tokenized is a None type. Defaulting to blank string.\n",
      "The text to be tokenized is a None type. Defaulting to blank string.\n",
      "The text to be tokenized is a None type. Defaulting to blank string.\n",
      "The text to be tokenized is a None type. Defaulting to blank string.\n",
      "The text to be tokenized is a None type. Defaulting to blank string.\n",
      "The text to be tokenized is a None type. Defaulting to blank string.\n",
      "The text to be tokenized is a None type. Defaulting to blank string.\n",
      "The text to be tokenized is a None type. Defaulting to blank string.\n",
      "The text to be tokenized is a None type. Defaulting to blank string.\n",
      "The text to be tokenized is a None type. Defaulting to blank string.\n",
      "The text to be tokenized is a None type. Defaulting to blank string.\n",
      "The text to be tokenized is a None type. Defaulting to blank string.\n",
      "The text to be tokenized is a None type. Defaulting to blank string.\n",
      "The text to be tokenized is a None type. Defaulting to blank string.\n",
      "The text to be tokenized is a None type. Defaulting to blank string.\n",
      "The text to be tokenized is a None type. Defaulting to blank string.\n",
      "The text to be tokenized is a None type. Defaulting to blank string.\n",
      "The text to be tokenized is a None type. Defaulting to blank string.\n",
      "The text to be tokenized is a None type. Defaulting to blank string.\n"
     ]
    }
   ],
   "source": [
    "# print(type(test_data['description']))\n",
    "# print(type(test_data.at['description',4729]))\n",
    "# test_data['description'].dropna(inplace=True)\n",
    "# train_data['description'].dropna(inplace=True)\n",
    "# print(train_data['description'].iloc[4729])\n",
    "# test_tokenized = test_data.apply(lambda r: w2v_tokenize_text(r['description']), axis=1).values\n",
    "# train_tokenized = train_data.apply(lambda r: w2v_tokenize_text(r['description']), axis=1).values\n",
    "# test_tokenized = df.description.apply(nltk.word_tokenize)\n",
    "\n",
    "# train_data.dropna(subset=['description'])\n",
    "# test_data.dropna(subset=['description'])\n",
    "test_tokenized = test_data[\"description\"].fillna(\"\").map(w2v_tokenize_text)\n",
    "train_tokenized = train_data[\"description\"].fillna(\"\").map(w2v_tokenize_text)\n",
    "\n",
    "\n",
    "# df.sample(5)\n",
    "# na.omit(dat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\gx\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:9: DeprecationWarning: Call to deprecated `syn0norm` (Attribute will be removed in 4.0.0, use self.vectors_norm instead).\n",
      "  if __name__ == '__main__':\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 11.7 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "X_train_word_average = word_averaging_list(model,train_tokenized)\n",
    "X_test_word_average = word_averaging_list(model,test_tokenized)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linear Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.05163631616800457"
      ]
     },
     "execution_count": 140,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reg = LinearRegression().fit(X_train_word_average, train_data['views'])\n",
    "# test_data_features = count_vectorizer.transform(test_data['description'].values.astype('U')) #3000*train_feature\n",
    "reg.score(X_test_word_average, test_data['views'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gradient Boost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7079379039078945"
      ]
     },
     "execution_count": 141,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "regr2 = GradientBoostingRegressor(n_estimators=500, random_state=0,learning_rate=0.1,loss='ls')\n",
    "regr2.fit(X_train_word_average, train_data['views'])\n",
    "regr2.score(X_test_word_average, test_data['views'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\gx\\Anaconda3\\lib\\site-packages\\sklearn\\svm\\base.py:193: FutureWarning: The default value of gamma will change from 'auto' to 'scale' in version 0.22 to account better for unscaled features. Set gamma explicitly to 'auto' or 'scale' to avoid this warning.\n",
      "  \"avoid this warning.\", FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LibSVM]"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.49865689865689866"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf = SVC(verbose=True)\n",
    "clf.fit(X_train_word_average, train_y) \n",
    "clf.score(X_test_word_average, test_y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7875457875457875"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf = GradientBoostingClassifier(n_estimators=500, random_state=0,learning_rate=0.1,loss='deviance')\n",
    "clf.fit(X_train_word_average, train_y) \n",
    "clf.score(X_test_word_average, test_y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\gx\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:9: DeprecationWarning: Call to deprecated `syn0norm` (Attribute will be removed in 4.0.0, use self.vectors_norm instead).\n",
      "  if __name__ == '__main__':\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "'NoneType' object is not subscriptable",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-74-86b76a6c755f>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     27\u001b[0m \u001b[1;31m# # X_test_word = word_list(model,test_tokenized)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     28\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 29\u001b[1;33m \u001b[0mX_train_word\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mword_list\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'SHANTELL'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"'S\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'CHANNEL'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'https'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     30\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     31\u001b[0m \u001b[1;31m# print(train_tokenized[8])\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-74-86b76a6c755f>\u001b[0m in \u001b[0;36mword_list\u001b[1;34m(wv, text_list)\u001b[0m\n\u001b[0;32m     22\u001b[0m \u001b[1;32mdef\u001b[0m  \u001b[0mword_list\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mwv\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtext_list\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     23\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 24\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvstack\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mword_append\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mwv\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mreview\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mreview\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mtext_list\u001b[0m \u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     25\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     26\u001b[0m \u001b[1;31m# # X_train_word = word_list(model,train_tokenized)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-74-86b76a6c755f>\u001b[0m in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m     22\u001b[0m \u001b[1;32mdef\u001b[0m  \u001b[0mword_list\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mwv\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtext_list\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     23\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 24\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvstack\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mword_append\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mwv\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mreview\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mreview\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mtext_list\u001b[0m \u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     25\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     26\u001b[0m \u001b[1;31m# # X_train_word = word_list(model,train_tokenized)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-74-86b76a6c755f>\u001b[0m in \u001b[0;36mword_append\u001b[1;34m(wv, words)\u001b[0m\n\u001b[0;32m      7\u001b[0m         \u001b[1;32melif\u001b[0m \u001b[0mword\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mwv\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvocab\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[1;31m#             print(wv.vocab[word].index)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 9\u001b[1;33m             \u001b[0mmean\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mwv\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msyn0norm\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mwv\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvocab\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mword\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     10\u001b[0m             \u001b[0mall_words\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0madd\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mwv\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvocab\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mword\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     11\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: 'NoneType' object is not subscriptable"
     ]
    }
   ],
   "source": [
    "# def word_append(wv, words):\n",
    "#     all_words, mean = set(), []\n",
    "    \n",
    "#     for word in words:\n",
    "#         if isinstance(word, np.ndarray):\n",
    "#             mean.append(word)\n",
    "#         elif word in wv.vocab:\n",
    "# #             print(wv.vocab[word].index)\n",
    "#             mean.append(wv.syn0norm[wv.vocab[word].index])\n",
    "#             all_words.add(wv.vocab[word].index)\n",
    "\n",
    "#     if not mean:\n",
    "# #         logging.warning(\"cannot compute similarity with no input %s\", words)\n",
    "#         # FIXME: remove these examples in pre-processing\n",
    "#         return np.zeros(25,)\n",
    "\n",
    "#     mean = gensim.matutils.unitvec(np.array(mean)).astype(np.float32))\n",
    "#     print(mean)\n",
    "    \n",
    "#     return mean\n",
    "\n",
    "# def  word_list(wv, text_list):\n",
    "    \n",
    "#     return np.vstack([word_append(wv, review) for review in text_list ])\n",
    "\n",
    "# # # X_train_word = word_list(model,train_tokenized)\n",
    "# # # X_test_word = word_list(model,test_tokenized)\n",
    "\n",
    "# X_train_word = word_list(model,['SHANTELL', \"'S\", 'CHANNEL', 'https'])\n",
    "\n",
    "# print(train_tokenized[8])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(36854, 25)\n"
     ]
    },
    {
     "ename": "AssertionError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-79-2b4f3d80c583>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m    117\u001b[0m \u001b[0mtrain_loader\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mutils\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mDataLoader\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_dataset\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;31m# create your dataloader\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    118\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 119\u001b[1;33m \u001b[0mtest_dataset\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mutils\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTensorDataset\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtest_tensor_x\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mtest_tensor_y\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;31m# create your datset\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    120\u001b[0m \u001b[0mtest_loader\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mutils\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mDataLoader\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtest_dataset\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;31m# create your dataloader if you get a error when loading test data you can set a batch_size here as well like train_dataloader\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    121\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\torch\\utils\\data\\dataset.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, *tensors)\u001b[0m\n\u001b[0;32m    156\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    157\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__init__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0mtensors\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 158\u001b[1;33m         \u001b[1;32massert\u001b[0m \u001b[0mall\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtensors\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m==\u001b[0m \u001b[0mtensor\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mtensor\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mtensors\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    159\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtensors\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtensors\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    160\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAssertionError\u001b[0m: "
     ]
    }
   ],
   "source": [
    "BATCH_SIZE = 50\n",
    "import torch.utils.data as utils\n",
    "import torch.optim as optim\n",
    "from torchsummary import summary\n",
    "\n",
    "class RNN(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.rnn=torch.nn.LSTM(\n",
    "            input_size=25,\n",
    "            hidden_size=64,\n",
    "            num_layers=1,\n",
    "            batch_first=True\n",
    "        )\n",
    "        self.out=torch.nn.Linear(in_features=64,out_features=3)\n",
    "\n",
    "    def forward(self,x):\n",
    "        # 一下关于shape的注释只针对单项\n",
    "        # output: [batch_size, time_step, hidden_size]\n",
    "        # h_n: [num_layers,batch_size, hidden_size] # 虽然LSTM的batch_first为True,但是h_n/c_n的第一维还是num_layers\n",
    "        # c_n: 同h_n\n",
    "        output,(h_n,c_n)=self.rnn(x)\n",
    "        print(output.size())\n",
    "        # output_in_last_timestep=output[:,-1,:] # 也是可以的\n",
    "        output_in_last_timestep=h_n[-1,:,:]\n",
    "        # print(output_in_last_timestep.equal(output[:,-1,:])) #ture\n",
    "        x=self.out(output_in_last_timestep)\n",
    "        return x\n",
    "    \n",
    "def train(model, device, train_loader, optimizer, epoch):\n",
    "    model.train()\n",
    "    correct = 0\n",
    "    for batch_idx, (data, target) in enumerate(train_loader):\n",
    "        data, target = data.to(device), target.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        output = model(data)\n",
    "        # print(\"shape:\",output.shape)\n",
    "        loss = F.cross_entropy(output, target)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        if batch_idx % 100 == 0: #Print loss every 100 batch\n",
    "            print('Train Epoch: {}\\tLoss: {:.6f}'.format(\n",
    "                epoch, loss.item()))\n",
    "    accuracy = test(model, device, train_loader)\n",
    "    return accuracy    \n",
    "\n",
    "def test(model, device, test_loader):\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    with torch.no_grad():\n",
    "        for data, target in test_loader:\n",
    "            data, target = data.to(device), target.to(device)\n",
    "            output = model(data)\n",
    "            pred = output.argmax(dim=1, keepdim=True)  # get the index of the max log-probability\n",
    "            correct += pred.eq(target.view_as(pred)).sum().item()\n",
    "\n",
    "    accuracy = 100. * correct / len(test_loader.dataset)\n",
    "    return accuracy\n",
    "\n",
    "# #Dimensions of output of neural network is (seq_len, batch , hidden_dim). Since we want output dimensions to be\n",
    "# #the same as n_letters, hidden_dim = n_letters (output dimensions = hidden_dimensions)\n",
    "# hidden_dim = n_letters     \n",
    "# #Invoking model. Input dimensions = n_letters i.e 28. output dimensions = hidden_dimensions = 28\n",
    "# model = MyLSTM(n_letters,hidden_dim)\n",
    "# #I'm using Adam optimizer here\n",
    "# optimizer = torch.optim.Adam(params = model.parameters(),lr=0.01)\n",
    "# #Loss function is CrossEntropyLoss\n",
    "# LOSS = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "# #List to store targets\n",
    "# targets = []\n",
    "# #Iterate through all chars in the sequence, starting from second letter. Since output for 1st letter is the 2nd letter\n",
    "# for x in data[1:]+'#':\n",
    "#     #Find the target index. For a, it is 0, For 'b' it is 1 etc..\n",
    "#     targets.append(letters.find(x))\n",
    "# #Convert into tensor\n",
    "# targets = torch.tensor(targets)\n",
    "\n",
    "# #List to store input\n",
    "# inpl = []\n",
    "# #Iterate through all inputs in the sequence\n",
    "# for c in data:\n",
    "#     #Convert into tensor\n",
    "#     inpl.append(ltt(c))\n",
    "# #Convert list to tensor\n",
    "# inp = torch.cat(inpl,dim=0)\n",
    "# #Reshape tensor into 3 dimensions (sequence length, batches = 1, dimensions = n_letters (28))\n",
    "# inp = inp.view(seq_len,1,n_letters)\n",
    "\n",
    "\n",
    "torch.manual_seed(1)\n",
    "np.random.seed(1)\n",
    "# Training settings\n",
    "use_cuda = True # Switch to False if you only want to use your CPU\n",
    "learning_rate = 0.01\n",
    "NumEpochs = 10\n",
    "batch_size = 32\n",
    "\n",
    "device = \"cpu\"#torch.device(\"cuda\" if use_cuda else \"cpu\")\n",
    "\n",
    "train_X = train_data_features\n",
    "# train_Y = train_Y.values\n",
    "test_X = test_data_features\n",
    "# test_Y = test_Y.values\n",
    "\n",
    "train_X = X_train_word_average.reshape([-1,25])#train_X.reshape([1,1,3000]) # the data is flatten so we reshape it here to get to the original dimensions of images\n",
    "test_X = X_train_word_average.reshape([-1,25])#test_X.reshape([1,1,3000])\n",
    "print(train_X.shape)\n",
    "# transform to torch tensors\n",
    "tensor_x = torch.tensor(train_X, device=device)#.todense()\n",
    "tensor_y = torch.tensor(train_Y, dtype=torch.long, device=device)\n",
    "\n",
    "test_tensor_x = torch.tensor(test_X, device=device)\n",
    "test_tensor_y = torch.tensor(test_Y, dtype=torch.long)\n",
    "\n",
    "train_dataset = utils.TensorDataset(tensor_x,tensor_y) # create your datset\n",
    "train_loader = utils.DataLoader(train_dataset, batch_size=batch_size) # create your dataloader\n",
    "\n",
    "test_dataset = utils.TensorDataset(test_tensor_x,test_tensor_y) # create your datset\n",
    "test_loader = utils.DataLoader(test_dataset) # create your dataloader if you get a error when loading test data you can set a batch_size here as well like train_dataloader\n",
    "\n",
    "model = RNN().to(device)\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate, weight_decay=0.0001)\n",
    "trainHis = []\n",
    "testHis = []\n",
    "# summary(model, (3000,1))\n",
    "for epoch in range(NumEpochs):\n",
    "    train_acc = train(model, device, train_loader, optimizer, epoch)\n",
    "    print('\\nTrain set Accuracy: {:.0f}%\\n'.format(train_acc))\n",
    "    test_acc = test(model, device, test_loader)\n",
    "    print('\\nTest set Accuracy: {:.0f}%\\n'.format(test_acc))\n",
    "    trainHis.append(train_acc)\n",
    "    testHis.append(test_acc)\n",
    "torch.save(model.state_dict(), \"mnist_cnn.pt\")\n",
    "\n",
    "#TODO: Plot train and test accuracy vs epoch\n",
    "plt.plot( range(NumEpochs), trainHis, label=\"train\")\n",
    "# plt.plot( epochs, acc_his['val'], label=\"validation\")\n",
    "plt.plot( range(NumEpochs), testHis, label=\"test\")\n",
    "plt.xlabel('epoch')\n",
    "plt.ylabel('accuracy')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# data_csv = pd.read_csv('./data.csv',usecols=[1])\n",
    "# plt.plot(data_csv)\n",
    "\n",
    "# 数据预处理\n",
    "# data_csv = data_csv.dropna()  # 滤除缺失数据\n",
    "# dataset = data_csv.values   # 获得csv的值\n",
    "# dataset = dataset.astype('float32')\n",
    "# max_value = np.max(dataset)  # 获得最大值\n",
    "# min_value = np.min(dataset)  # 获得最小值\n",
    "# scalar = max_value - min_value  # 获得间隔数量\n",
    "# dataset = list(map(lambda x: x / scalar, dataset)) # 归一化\n",
    "\n",
    "# def create_dataset(dataset, look_back=2):\n",
    "#     dataX, dataY = [], []\n",
    "#     for i in range(len(dataset) - look_back):\n",
    "#         a = dataset[i:(i + look_back)]\n",
    "#         dataX.append(a)\n",
    "#         dataY.append(dataset[i + look_back])\n",
    "#     return np.array(dataX), np.array(dataY)\n",
    "\n",
    "# 创建好输入输出\n",
    "# data_X, data_Y = create_dataset(dataset)\n",
    "\n",
    "# 划分训练集和测试集，70% 作为训练集\n",
    "# train_size = int(len(data_X) * 0.7)\n",
    "# test_size = len(data_X) - train_size\n",
    "# train_X = data_X[:train_size]\n",
    "# train_Y = data_Y[:train_size]\n",
    "# test_X = data_X[train_size:]\n",
    "# test_Y = data_Y[train_size:]\n",
    "\n",
    "import torch\n",
    "\n",
    "train_y, test_y = train_test_split(y_train, test_size=0.1, random_state=42)\n",
    "\n",
    "train_X = X_train_word_average.reshape(-1, 1, 25)\n",
    "train_Y = train_y.reshape(-1, 1, 1)\n",
    "test_X = X_test_word_average.reshape(-1, 1, 25)\n",
    "\n",
    "train_x = torch.from_numpy(train_X)\n",
    "train_y = torch.from_numpy(train_Y)\n",
    "test_x = torch.from_numpy(test_X)\n",
    "\n",
    "from torch import nn\n",
    "from torch.autograd import Variable\n",
    "\n",
    "class lstm(nn.Module):\n",
    "    def __init__(self,input_size=25,hidden_size=50,output_size=1,num_layer=2):\n",
    "        super(lstm,self).__init__()\n",
    "        self.layer1 = nn.LSTM(input_size,hidden_size,num_layer)\n",
    "        elf.gru = nn.GRU(input_dim, hidden_dim, n_layers, batch_first=True, dropout=drop_prob)\n",
    "        self.layer2 = nn.Linear(hidden_size,output_size)\n",
    "    \n",
    "    def forward(self,x):\n",
    "        x,_ = self.layer1(x.float())\n",
    "        s,b,h = x.size()\n",
    "        x = x.view(s*b,h)\n",
    "        x = self.layer2(x)\n",
    "        x = x.view(s,b,-1)\n",
    "        return x\n",
    "\n",
    "model = lstm(25, 50,1,2)\n",
    "\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-2)\n",
    "\n",
    "# 开始训练\n",
    "for e in range(1000):\n",
    "    var_x = Variable(train_x)\n",
    "    var_y = Variable(train_y.float())\n",
    "    # 前向传播\n",
    "    out = model(var_x)\n",
    "    loss = criterion(out, var_y)\n",
    "    # 反向传播\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "    if (e + 1) % 100 == 0: # 每 100 次输出结果\n",
    "        print('Epoch: {}, Loss: {:.5f}'.format(e + 1, loss.data[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
